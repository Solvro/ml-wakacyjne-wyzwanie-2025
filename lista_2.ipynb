{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lista 2 - trenowanie modelu\n",
        "Wykorzystamy w tym celu zbiór danych przygotowany w liście 1"
      ],
      "metadata": {
        "id": "Vj57rLp-tUNd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUip6XKGtRhU",
        "outputId": "4e18e6dc-70c2-496c-9d97-096c9e8a0f57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Import bibliotek:\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "# przygotowane dane + etykiety z poprzedniej listy pobrałem do pliku csv\n",
        "X = pd.read_csv(\"/content/drive/MyDrive/ml kurs/titanic_features.csv\") # przygotowane cechy\n",
        "y = pd.read_csv(\"/content/drive/MyDrive/ml kurs/titanic_labels.csv\").squeeze() # wektor etykiet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zacznijmy od zrobienia dummy classifiera"
      ],
      "metadata": {
        "id": "kZ4Q6yXQDRUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy = DummyClassifier(strategy=\"most_frequent\", random_state=42)\n",
        "dummy.fit(X,y)\n",
        "dummy.predict(X),dummy.score(X,y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZU7paYiDVKG",
        "outputId": "fb3db6e4-a8d8-4860-97f4-65f6b265c9e7"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 0.589825119236884)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 0.5898, sprawdźmy inne strategie bo most frequent zakłada że nastąpi najczęstsza etykieta (czyli zgon)"
      ],
      "metadata": {
        "id": "nY6BXcSMFlTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy = DummyClassifier(strategy=\"stratified\", random_state=42)\n",
        "dummy.fit(X,y)\n",
        "dummy.predict(X),dummy.score(X,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQ_SfGQpF5qR",
        "outputId": "374428aa-7837-430e-d409-d6f450a5d37d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
              "        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
              "        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
              "        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
              "        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
              "        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
              "        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
              "        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
              "        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
              "        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
              "        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,\n",
              "        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
              "        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
              "        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,\n",
              "        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
              "        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,\n",
              "        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,\n",
              "        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,\n",
              "        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
              "        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,\n",
              "        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
              "        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
              "        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0]),\n",
              " 0.5373608903020668)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wynik jest gorszy, zostawiamy 0.589825119236884 jako bazę porównawczą\n",
        "naszego modelu"
      ],
      "metadata": {
        "id": "3E83J_EJGX5U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zanim zaczniemy tworzyć modele, przygotujemy funkcję do oceniania ich oraz podzielimy zbiór na treningowy i testowy"
      ],
      "metadata": {
        "id": "fIcNiRmeK-9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42) # dzielimy zbiór w stosunku 80% trening, 20% test\n",
        "\n",
        "def eval_model(model, X_train, y_train, X_test, y_test):\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    y_pred_test = model.predict(X_test)\n",
        "\n",
        "\n",
        "    def proba_or_score(m, X):     # ujednolicenie wyników punktowych dla ROC\n",
        "        if hasattr(m, \"predict_proba\"):\n",
        "            return m.predict_proba(X)[:, 1]\n",
        "        elif hasattr(m, \"decision_function\"):\n",
        "            return m.decision_function(X)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    s_train = proba_or_score(model, X_train)\n",
        "    s_test = proba_or_score(model, X_test)\n",
        "\n",
        "    metrics = {\n",
        "        \"acc_train\": accuracy_score(y_train, y_pred_train),\n",
        "        \"acc_test\": accuracy_score(y_test, y_pred_test),\n",
        "        \"prec_train\": precision_score(y_train, y_pred_train, zero_division=0),\n",
        "        \"prec_test\": precision_score(y_test, y_pred_test, zero_division=0),\n",
        "        \"f1_train\": f1_score(y_train, y_pred_train, zero_division=0),\n",
        "        \"f1_test\": f1_score(y_test, y_pred_test, zero_division=0),\n",
        "        \"roc_train\": roc_auc_score(y_train, s_train) if s_train is not None else np.nan,\n",
        "        \"roc_test\": roc_auc_score(y_test, s_test) if s_test is not None else np.nan,\n",
        "    }\n",
        "    return metrics,y_pred_test"
      ],
      "metadata": {
        "id": "QXI7fBbYJdjS"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model nr. 1 - Random forest\n",
        "dla hiperparametrów ograniczymy się do 2, max_depth oraz n_estimators\n",
        "przetestujemy kilka wariacji tych hiperparametrów więc wykorzystamy gridsearch, aby znaleźć te optymalne"
      ],
      "metadata": {
        "id": "Sz-8DKlhHBj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r_forest_pipe = Pipeline([\n",
        "    (\"clf\", RandomForestClassifier(random_state=42))\n",
        "])\n",
        "r_forest_grid = {\n",
        "    \"clf__n_estimators\": [100, 300, 600, 900],\n",
        "    \"clf__max_depth\": [None, 5, 10, 20, 30],\n",
        "}\n",
        "\n",
        "r_forest_gs = GridSearchCV(\n",
        "    r_forest_pipe,\n",
        "    param_grid=r_forest_grid,\n",
        "    scoring=\"f1\",\n",
        "    cv=5, # 4 do treningu, 1 do testu\n",
        "    refit=True,\n",
        ")\n",
        "\n",
        "r_forest_gs.fit(X_train, y_train)\n",
        "print(\"RandomForest - najlepsze parametry:\", r_forest_gs.best_params_)\n",
        "r_forest_best = r_forest_gs.best_estimator_\n",
        "r_forest_metrics,y_pred_forest = eval_model(r_forest_best, X_train, y_train, X_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBQshxffGkPt",
        "outputId": "9c8d488c-922b-4670-e4a0-4b6d815c50cf"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForest - najlepsze parametry: {'clf__max_depth': None, 'clf__n_estimators': 900}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r_forest_metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkjwdSq-OIYT",
        "outputId": "55610026-2b3a-4f3d-dd8d-ff86a14eb515"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'acc_train': 0.9264413518886679,\n",
              " 'acc_test': 0.7777777777777778,\n",
              " 'prec_train': 0.9470899470899471,\n",
              " 'prec_test': 0.7727272727272727,\n",
              " 'f1_train': 0.9063291139240506,\n",
              " 'f1_test': 0.7083333333333334,\n",
              " 'roc_train': np.float64(0.9772563825961884),\n",
              " 'roc_test': np.float64(0.8508316008316009)}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "wyniki niewątpliwie są lepsze w porównaniu do dummy, nie mniej widać tendencję spadkową ze zbioru treningowego do testowego, prawdopodobnie poprzez overfitting. Moglibyśmy jeszcze kombinować z ustawianiem innych parametrów, ale już przy tak niewielkiej liczbie kod ładuje się relatywnie długo, dla cierpliwych lub tych z lepszym sprzętem można r_forest_grid poszerzyć o dodatkowe hiperparametry i dłuższe listy z ich wartościami"
      ],
      "metadata": {
        "id": "yrIHWwDYPdTq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model nr.2 - SVM\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CeApgd19QPno"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tak samo jak dla lasu losowego, przeprowadzimy siatkę potencjalnych parametrów"
      ],
      "metadata": {
        "id": "jXahiasoRRYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm_pipe = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", SVC(kernel=\"rbf\", probability=True, random_state=42))\n",
        "])\n",
        "\n",
        "svm_param_grid = {\n",
        "    \"clf__C\": [0.1, 1, 10, 100],\n",
        "    \"clf__gamma\": [0.001, 0.01, 0.1, 1.0],\n",
        "}\n",
        "\n",
        "svm_gs = GridSearchCV(\n",
        "    svm_pipe,\n",
        "    param_grid=svm_param_grid,\n",
        "    scoring=\"f1\",\n",
        "    cv=5,\n",
        "    refit=True,\n",
        "    return_train_score=True,\n",
        ")\n",
        "svm_gs.fit(X_train, y_train)\n",
        "\n",
        "print(\"SVM - najlepsze parametry:\", svm_gs.best_params_)\n",
        "svm_best = svm_gs.best_estimator_\n",
        "svm_metrics,y_pred_svm = eval_model(svm_best, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwnEvtgeRWXL",
        "outputId": "95d3178f-ae6d-411e-e4ec-d42e95f1f476"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM - najlepsze parametry: {'clf__C': 100, 'clf__gamma': 0.1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "svm_metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bz3d_7CHRonH",
        "outputId": "f51fe035-952d-4652-f18f-a758ebb26d1a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'acc_train': 0.8330019880715706,\n",
              " 'acc_test': 0.8174603174603174,\n",
              " 'prec_train': 0.8674698795180723,\n",
              " 'prec_test': 0.8918918918918919,\n",
              " 'f1_train': 0.7741935483870968,\n",
              " 'f1_test': 0.7415730337078652,\n",
              " 'roc_train': np.float64(0.8932120558334151),\n",
              " 'roc_test': np.float64(0.8549896049896051)}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tutaj overfitting nie występuje, test wychodzi podobnie przewyższając jednocześnie wynik testów dla random forest. Precyzja jest tutaj na wyjątkowo wysokim poziomie więc model raczej nie tworzy fałszywych alarmów (jeśli zgadł że ktoś żyje to ma rację)"
      ],
      "metadata": {
        "id": "M5vtZJ4eR6Lf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wnioski:\n",
        "\n",
        "Random forest zdecydowanie ma większy potencjał do dawania wysokiej jakości modelu co pokazuje evaluacja zbioru treningowego jednak overfitting mocno podcina mu skrzydła i należałoby się nagimnastykować z ustawieniem odpowiednich hiperparametrów co jest pewnym obciążeniem. SVM z kolei w tej sytuacji jest bezpieczniejszym wyborem bo nie przeucza się i posiada stabilne wyniki.\n",
        "W celu osiągnięcia lepszych wyników modelu nielicząc wspomnianego majstrowania przy hiperparametrac możnaby sprawdzić inne modele takie jak regresja liniowa lub XGBoost i przeprowadzić identyczne działania na nich."
      ],
      "metadata": {
        "id": "Ze0I-vCQSsy9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zobaczmy czy przeżyłbym tę katastrofę"
      ],
      "metadata": {
        "id": "F92VpFM4WZPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ja = pd.DataFrame([{\n",
        "    \"Pclass\": 3,       # wiadomo najtaniej\n",
        "    \"Parch\": 2,        # Z rodzicami\n",
        "    \"Is male\": 1,      # chłop\n",
        "    \"Age_scaled\": 0.228  # 19 po zminmaxowaniu\n",
        "}])\n",
        "\n",
        "prediction = svm_best.predict(ja)[0]\n",
        "proba = svm_best.predict_proba(ja)[0,1]\n",
        "\n",
        "print(f\"\\nPrzewidywanie: {'przeżyje' if prediction==1 else 'nie przeżyje'}\")\n",
        "print(f\"Prawdopodobieństwo przeżycia: {proba:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgfZi1mPWged",
        "outputId": "33c1ce64-b3ba-448c-f288-02c2a517d9eb"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Przewidywanie: nie przeżyje\n",
            "Prawdopodobieństwo przeżycia: 0.212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MHM"
      ],
      "metadata": {
        "id": "FL0oshimcYuu"
      }
    }
  ]
}