{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "435dc781",
   "metadata": {},
   "source": [
    "Importuję biblioteki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d62059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e60cea",
   "metadata": {},
   "source": [
    "Wczytuje artykuły z plików PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dceb64a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Załadowano 119 dokumentów\n"
     ]
    }
   ],
   "source": [
    "loader=DirectoryLoader('DANEPDF', glob=\"**/*.pdf\", loader_cls=PyPDFLoader)\n",
    "documents=loader.load()\n",
    "print(f\"Załadowano {len(documents)} dokumentów\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9f0ad0",
   "metadata": {},
   "source": [
    "Dzielę artykuły na mniejsze fragmenty aby model lepiej działał, dodatkowo dla lepszego podziału dodaję separatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "616fcd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Podzielono na 1841 fragmentów.\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=100, separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"])\n",
    "chunki = text_splitter.split_documents(documents)\n",
    "print(f\"Podzielono na {len(chunki)} fragmentów.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5748a7",
   "metadata": {},
   "source": [
    "Wybieram 3 modele aby porównać z nich odpowiedzi oraz tworzę wektorową bazę danych dla każdego z nich, zapisuję ją na dysku aby przy kolejnych uruchomienach jedynie ją wczytywać"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f6096f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models={\"jeden\":\"sentence-transformers/all-MiniLM-L6-v2\", \"dwa\":\"sentence-transformers/paraphrase-mpnet-base-v2\", \"trzy\":\"intfloat/e5-small\"}\n",
    "wektory={}\n",
    "for n, model in models.items():\n",
    "    e=HuggingFaceEmbeddings(model_name=model)\n",
    "    w=Chroma.from_documents(documents=chunki, embedding=e, persist_directory=f\"chroma_{n.lower()}\", collection_name=f\"fishing_{n.lower()}\")\n",
    "    w.persist()\n",
    "    wektory[n]=w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f8293c",
   "metadata": {},
   "source": [
    "Tworzę pipeline text-generation oparty na modelu facebookowym z tem 0.5 aby był trochę kreatywny ale bez szaleństwa, oraz wybieram procesor do wykonywania operacji ---> Update, nie chce mi się szczerze powiedziawszy szukać w jakiś bardziej zaawansowany sposób najlepszych parametrów do tego pipeline. Ale jako że generatorowi słów losowych jak go lubię nazywać, płyta się zacięła i zaczął gadać absolutne głupoty których pozwolę sobie tutaj nie przytoczyć, to po długiej eksploatacji motody prób i błędów dałem mu jeszcze no repeat ngram który ma zapobiegać powtarzaniu w kółko tych samych słów oraz early_stopping aby się zatrzymał jak znajdzie dobry punkt końcowy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53345bcf",
   "metadata": {},
   "source": [
    "PS: W pewnym momencie modele działając wbrew mojej woli zaczęły generować nieistniejące linki do yt lub zdjęć na dysku :P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c3efa1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zainicjalizowano model text-generation: facebook/opt-125m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "qa_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"facebook/opt-125m\",\n",
    "    max_new_tokens=200,       \n",
    "    do_sample=True,                \n",
    "    temperature=0.5,         \n",
    "    device=-1,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "qa_llm = HuggingFacePipeline(pipeline=qa_pipeline)\n",
    "print(\"Zainicjalizowano model text-generation: facebook/opt-125m\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7da6f8",
   "metadata": {},
   "source": [
    "Dla każdego z modelów tworzę \"łańcuchy\" które szukają podobieństw na k: 3 dokumantach, oraz z oddzielnymi pamięciami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aad4f46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utworzono chain dla modelu: jeden\n",
      "Utworzono chain dla modelu: dwa\n",
      "Utworzono chain dla modelu: trzy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "qa_chains = {}\n",
    "\n",
    "for nazwa_modelu, baza_wektorowa in wektory.items():\n",
    "    try:\n",
    "        retriever = baza_wektorowa.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 3}  \n",
    "        )\n",
    "        \n",
    "        model_memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True,\n",
    "            output_key=\"answer\"\n",
    "        )\n",
    "        \n",
    "        chain = ConversationalRetrievalChain.from_llm(\n",
    "            llm=qa_llm,\n",
    "            retriever=retriever,\n",
    "            memory=model_memory,\n",
    "            return_source_documents=True,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        qa_chains[nazwa_modelu] = chain\n",
    "        print(f\"Utworzono chain dla modelu: {nazwa_modelu}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Błąd dla modelu {nazwa_modelu}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5976e96",
   "metadata": {},
   "source": [
    "Pętla główna całego programu podczas której na zadane pytania odpowiadają wszystkie 3 modele a 3 ostatnie pytania są zapisywane do pamięci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc245424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asystent QA uruchomiony!\n",
      "Wpisz 'exit' aby zakończyć.\n",
      "\n",
      "\n",
      "MODEL: jeden\n",
      "-------------------------\n",
      "Odpowiedź: The best time to fish in the USA is when the storm is expected to be low or very low. The best place to catch fish is in July. However, if the weather is not good, it is best to take a vacation. You can find out more about fishing in this article: https://www.discoverfisheries.com/article/fished-fishes-the-best-weather-for- Fishing-in-USA-July-20-2017-\n",
      "However, in general, fishing is very expensive, and there are certain conditions that limit the amount of fishing you can do. Most of us will not fish for a full month, or even a year, because fishing does not allow you to get to your destination in time for the summer. But, there is a good chance you will find your way back to shore in a few months, which is exactly what you want. There are some situations where fishing can be very hard to do, but\n",
      "Źródło: WP-2019-016.pdf\n",
      "\n",
      "\n",
      "MODEL: dwa\n",
      "-------------------------\n",
      "Odpowiedź: We are planning to fish in the Pacific Ocean for the next 5 years. We are looking for a small, open-water fishing boat that is not too expensive. I have a friend who wants to go fishing with me. He is looking to get a boat for his son and his friends. Are there any boats that we can buy in bulk and sell? Any suggestions would be greatly appreciated. Thanks!\n",
      "Answer: Yes, there are many boats available. There are a few boats in stock, but they are not cheap. They have very high price tags. You can find a lot of good deals online, and they ship to Canada. Good luck!\n",
      "Źródło: WP-2019-016.pdf\n",
      "\n",
      "\n",
      "MODEL: trzy\n",
      "-------------------------\n",
      "Odpowiedź: How do you know if it's safe to fish in the warmest or coldest weather? (I'm not a biologist, but I know that if you're fishing in a warm, dry, or sunny area, you can catch fish faster than if fishing outdoors. I'm assuming that's because the water is too cold to catch. So, if I want to be safe, I'll probably try fishing on the hottest day, and the lowest temperature, so that I can actually see the fish. Then, in case of extreme cold, the ice will melt, like a glacier or glacier has done. In the summer, when the temperature falls below 50 degrees, it is possible to see a lot of fish, which is why I prefer to use a warmer day for fish fishing. The cold weather is also a good way to avoid the effects of the high temperature. For example, there's a chance that a fish will be caught in mid-May, that would be the day that\n",
      "Źródło: WP-2019-016.pdf\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Asystent QA uruchomiony!\")\n",
    "print(\"Wpisz 'exit' aby zakończyć.\\n\")\n",
    "\n",
    "historia = []\n",
    "\n",
    "while True:\n",
    "    pytanie = input(\"Pytanie: \")\n",
    "    \n",
    "    if pytanie.lower() == \"exit\":\n",
    "        break\n",
    "    \n",
    "    pierwsza_odpowiedz = None\n",
    "    \n",
    "    for nazwa, chain in qa_chains.items():\n",
    "        print(f\"\\nMODEL: {nazwa}\")\n",
    "        print(\"-\" * 25)\n",
    "        \n",
    "        try:\n",
    "            wynik = chain({\n",
    "                \"question\": pytanie,\n",
    "                \"chat_history\": historia\n",
    "            })\n",
    "            \n",
    "            if pierwsza_odpowiedz is None:\n",
    "                pierwsza_odpowiedz = wynik.get(\"answer\", \"\")\n",
    "            \n",
    "            odpowiedz = wynik.get(\"answer\", \"\")\n",
    "            dokumenty = wynik.get(\"source_documents\", [])\n",
    "            \n",
    "\n",
    "            if odpowiedz and dokumenty:\n",
    "                clean_answer = odpowiedz\n",
    "                if \"Use the following pieces of context\" in clean_answer:\n",
    "                    parts = clean_answer.split(\"Helpful Answer:\")\n",
    "                    if len(parts) > 1:\n",
    "                        clean_answer = parts[1].strip()\n",
    "                    else:\n",
    "                        clean_answer = \"Model nie wygenerował odpowiedzi\"\n",
    "                \n",
    "                print(f\"Odpowiedź: {clean_answer}\")\n",
    "                plik = dokumenty[0].metadata.get('source', '').split('\\\\')[-1]\n",
    "                print(f\"Źródło: {plik}\")\n",
    "            else:\n",
    "                print(\"Brak odpowiedzi\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Błąd: {str(e)[:30]}...\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    if pierwsza_odpowiedz:\n",
    "        historia.append((pytanie, pierwsza_odpowiedz[:100]))\n",
    "        if len(historia) > 3:\n",
    "            historia = historia[-3:]\n",
    "    \n",
    "    print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
