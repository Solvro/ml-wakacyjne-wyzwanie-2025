{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80b64ed6",
   "metadata": {},
   "source": [
    "Importuję wszystko czego będę potrzebować"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d64a7080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4f7309",
   "metadata": {},
   "source": [
    "Pobieram dane z datasetu i przerabiam je na tensory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92e8b29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "train_data=datasets.MNIST(\n",
    "    root=\"dane\",\n",
    "    train=True,\n",
    "    transform=ToTensor(),\n",
    "    download=True,\n",
    ")\n",
    "test_data=datasets.MNIST(\n",
    "    root=\"dane\",\n",
    "    train=False,\n",
    "    transform=ToTensor(),\n",
    "    download=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e66ada",
   "metadata": {},
   "source": [
    "Sprawdzam co pobrałem i okazuje się że to 70k obrazków 28x28px z liczbami 0-9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71c8d8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e848ae7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3265b145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4,  ..., 5, 6, 8])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f092b4",
   "metadata": {},
   "source": [
    "Tworzę słownik z train i test data loaderami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69c2968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders={\n",
    "    'train': DataLoader(train_data, batch_size=100, shuffle=True, num_workers=1),\n",
    "    'test': DataLoader(test_data, batch_size=100, shuffle=False, num_workers=1),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5558a0f7",
   "metadata": {},
   "source": [
    "Towrzę funkcję init w której mam 2 warstwy konwolucyjne i 2 FC a następnie funkcję forward która wykrywa cechy służące rozpoznawaniu liczb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "202d0f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1=nn.Conv2d(1,10, kernel_size=5)\n",
    "        self.conv2=nn.Conv2d(10,20, kernel_size=5)\n",
    "        self.conv2_drop=nn.Dropout2d()\n",
    "        self.fc1=nn.Linear(320, 50)\n",
    "        self.fc2=nn.Linear(50, 10)\n",
    "    def forward(self, X):\n",
    "        X=F.relu(F.max_pool2d(self.conv1(X), 2)) \n",
    "        X=F.relu(F.max_pool2d(self.conv2_drop(self.conv2(X)), 2))\n",
    "        X=X.view(-1, 320)\n",
    "        X=F.relu(self.fc1(X))\n",
    "        X=F.dropout(X, training=self.training)\n",
    "        X=self.fc2(X)\n",
    "        return F.softmax(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138a75cb",
   "metadata": {},
   "source": [
    "Zmieniam device na GPU jeśli to możliwe a następnie tworzę funkcje do trenowania i testowania modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf2bcf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model=CNN().to(device)\n",
    "\n",
    "optimizer=optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn=nn.CrossEntropyLoss()\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for idx, (data, target) in enumerate(loaders['train']):\n",
    "        data, target=data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output=model(data)\n",
    "        loss=loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if idx%20==0:\n",
    "            print(f\"Train Epoch: {epoch} [{idx * len(data)}/{len(loaders['train'].dataset)}] Loss: {loss.item():.6f}\")\n",
    "    \n",
    "def test():\n",
    "    model.eval()\n",
    "        \n",
    "    test_loss=0\n",
    "    correct=0\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for data, target in loaders['test']:\n",
    "            data, target=data.to(device), target.to(device)\n",
    "            output=model(data)\n",
    "            test_loss+=loss_fn(output, target).item()\n",
    "            pred=output.argmax(dim=1, keepdim=True)\n",
    "            correct+=pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss/=len(loaders['test'].dataset)  \n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(loaders[\"test\"].dataset)} ({100. * correct / len(loaders[\"test\"].dataset):.0f}%)\\n')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858af202",
   "metadata": {},
   "source": [
    "Trenuję i testuję model dla 11 epok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6342e9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pilcz\\AppData\\Local\\Temp\\ipykernel_9948\\2897722950.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000] Loss: 2.303375\n",
      "Train Epoch: 1 [2000/60000] Loss: 2.293479\n",
      "Train Epoch: 1 [4000/60000] Loss: 2.201348\n",
      "Train Epoch: 1 [6000/60000] Loss: 2.003416\n",
      "Train Epoch: 1 [8000/60000] Loss: 1.891096\n",
      "Train Epoch: 1 [10000/60000] Loss: 1.845345\n",
      "Train Epoch: 1 [12000/60000] Loss: 1.760636\n",
      "Train Epoch: 1 [14000/60000] Loss: 1.747607\n",
      "Train Epoch: 1 [16000/60000] Loss: 1.757898\n",
      "Train Epoch: 1 [18000/60000] Loss: 1.747696\n",
      "Train Epoch: 1 [20000/60000] Loss: 1.690272\n",
      "Train Epoch: 1 [22000/60000] Loss: 1.690586\n",
      "Train Epoch: 1 [24000/60000] Loss: 1.648023\n",
      "Train Epoch: 1 [26000/60000] Loss: 1.676679\n",
      "Train Epoch: 1 [28000/60000] Loss: 1.667831\n",
      "Train Epoch: 1 [30000/60000] Loss: 1.718425\n",
      "Train Epoch: 1 [32000/60000] Loss: 1.710046\n",
      "Train Epoch: 1 [34000/60000] Loss: 1.600942\n",
      "Train Epoch: 1 [36000/60000] Loss: 1.644489\n",
      "Train Epoch: 1 [38000/60000] Loss: 1.590836\n",
      "Train Epoch: 1 [40000/60000] Loss: 1.647428\n",
      "Train Epoch: 1 [42000/60000] Loss: 1.614542\n",
      "Train Epoch: 1 [44000/60000] Loss: 1.579713\n",
      "Train Epoch: 1 [46000/60000] Loss: 1.596182\n",
      "Train Epoch: 1 [48000/60000] Loss: 1.648236\n",
      "Train Epoch: 1 [50000/60000] Loss: 1.595793\n",
      "Train Epoch: 1 [52000/60000] Loss: 1.616953\n",
      "Train Epoch: 1 [54000/60000] Loss: 1.572297\n",
      "Train Epoch: 1 [56000/60000] Loss: 1.598185\n",
      "Train Epoch: 1 [58000/60000] Loss: 1.573021\n",
      "\n",
      "Test set: Average loss: 0.0153, Accuracy: 9335/10000 (93%)\n",
      "\n",
      "Train Epoch: 2 [0/60000] Loss: 1.588863\n",
      "Train Epoch: 2 [2000/60000] Loss: 1.636593\n",
      "Train Epoch: 2 [4000/60000] Loss: 1.615841\n",
      "Train Epoch: 2 [6000/60000] Loss: 1.624870\n",
      "Train Epoch: 2 [8000/60000] Loss: 1.590096\n",
      "Train Epoch: 2 [10000/60000] Loss: 1.555137\n",
      "Train Epoch: 2 [12000/60000] Loss: 1.576168\n",
      "Train Epoch: 2 [14000/60000] Loss: 1.595282\n",
      "Train Epoch: 2 [16000/60000] Loss: 1.619092\n",
      "Train Epoch: 2 [18000/60000] Loss: 1.598657\n",
      "Train Epoch: 2 [20000/60000] Loss: 1.634255\n",
      "Train Epoch: 2 [22000/60000] Loss: 1.586892\n",
      "Train Epoch: 2 [24000/60000] Loss: 1.571243\n",
      "Train Epoch: 2 [26000/60000] Loss: 1.563877\n",
      "Train Epoch: 2 [28000/60000] Loss: 1.610836\n",
      "Train Epoch: 2 [30000/60000] Loss: 1.572775\n",
      "Train Epoch: 2 [32000/60000] Loss: 1.569797\n",
      "Train Epoch: 2 [34000/60000] Loss: 1.561635\n",
      "Train Epoch: 2 [36000/60000] Loss: 1.586670\n",
      "Train Epoch: 2 [38000/60000] Loss: 1.608990\n",
      "Train Epoch: 2 [40000/60000] Loss: 1.572402\n",
      "Train Epoch: 2 [42000/60000] Loss: 1.591002\n",
      "Train Epoch: 2 [44000/60000] Loss: 1.554354\n",
      "Train Epoch: 2 [46000/60000] Loss: 1.582324\n",
      "Train Epoch: 2 [48000/60000] Loss: 1.581744\n",
      "Train Epoch: 2 [50000/60000] Loss: 1.562029\n",
      "Train Epoch: 2 [52000/60000] Loss: 1.625991\n",
      "Train Epoch: 2 [54000/60000] Loss: 1.539474\n",
      "Train Epoch: 2 [56000/60000] Loss: 1.567882\n",
      "Train Epoch: 2 [58000/60000] Loss: 1.565132\n",
      "\n",
      "Test set: Average loss: 0.0151, Accuracy: 9510/10000 (95%)\n",
      "\n",
      "Train Epoch: 3 [0/60000] Loss: 1.575055\n",
      "Train Epoch: 3 [2000/60000] Loss: 1.554730\n",
      "Train Epoch: 3 [4000/60000] Loss: 1.579311\n",
      "Train Epoch: 3 [6000/60000] Loss: 1.591133\n",
      "Train Epoch: 3 [8000/60000] Loss: 1.592105\n",
      "Train Epoch: 3 [10000/60000] Loss: 1.534218\n",
      "Train Epoch: 3 [12000/60000] Loss: 1.548591\n",
      "Train Epoch: 3 [14000/60000] Loss: 1.582105\n",
      "Train Epoch: 3 [16000/60000] Loss: 1.578848\n",
      "Train Epoch: 3 [18000/60000] Loss: 1.575226\n",
      "Train Epoch: 3 [20000/60000] Loss: 1.552218\n",
      "Train Epoch: 3 [22000/60000] Loss: 1.540328\n",
      "Train Epoch: 3 [24000/60000] Loss: 1.584645\n",
      "Train Epoch: 3 [26000/60000] Loss: 1.526894\n",
      "Train Epoch: 3 [28000/60000] Loss: 1.560617\n",
      "Train Epoch: 3 [30000/60000] Loss: 1.507733\n",
      "Train Epoch: 3 [32000/60000] Loss: 1.558929\n",
      "Train Epoch: 3 [34000/60000] Loss: 1.553233\n",
      "Train Epoch: 3 [36000/60000] Loss: 1.520353\n",
      "Train Epoch: 3 [38000/60000] Loss: 1.556313\n",
      "Train Epoch: 3 [40000/60000] Loss: 1.568223\n",
      "Train Epoch: 3 [42000/60000] Loss: 1.566772\n",
      "Train Epoch: 3 [44000/60000] Loss: 1.561105\n",
      "Train Epoch: 3 [46000/60000] Loss: 1.579346\n",
      "Train Epoch: 3 [48000/60000] Loss: 1.566883\n",
      "Train Epoch: 3 [50000/60000] Loss: 1.570431\n",
      "Train Epoch: 3 [52000/60000] Loss: 1.564028\n",
      "Train Epoch: 3 [54000/60000] Loss: 1.546348\n",
      "Train Epoch: 3 [56000/60000] Loss: 1.538172\n",
      "Train Epoch: 3 [58000/60000] Loss: 1.570093\n",
      "\n",
      "Test set: Average loss: 0.0150, Accuracy: 9593/10000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/60000] Loss: 1.578512\n",
      "Train Epoch: 4 [2000/60000] Loss: 1.561513\n",
      "Train Epoch: 4 [4000/60000] Loss: 1.564900\n",
      "Train Epoch: 4 [6000/60000] Loss: 1.595514\n",
      "Train Epoch: 4 [8000/60000] Loss: 1.586445\n",
      "Train Epoch: 4 [10000/60000] Loss: 1.531105\n",
      "Train Epoch: 4 [12000/60000] Loss: 1.553265\n",
      "Train Epoch: 4 [14000/60000] Loss: 1.604480\n",
      "Train Epoch: 4 [16000/60000] Loss: 1.582252\n",
      "Train Epoch: 4 [18000/60000] Loss: 1.511599\n",
      "Train Epoch: 4 [20000/60000] Loss: 1.549012\n",
      "Train Epoch: 4 [22000/60000] Loss: 1.566332\n",
      "Train Epoch: 4 [24000/60000] Loss: 1.542362\n",
      "Train Epoch: 4 [26000/60000] Loss: 1.495867\n",
      "Train Epoch: 4 [28000/60000] Loss: 1.524145\n",
      "Train Epoch: 4 [30000/60000] Loss: 1.561230\n",
      "Train Epoch: 4 [32000/60000] Loss: 1.566486\n",
      "Train Epoch: 4 [34000/60000] Loss: 1.538266\n",
      "Train Epoch: 4 [36000/60000] Loss: 1.565677\n",
      "Train Epoch: 4 [38000/60000] Loss: 1.562209\n",
      "Train Epoch: 4 [40000/60000] Loss: 1.577892\n",
      "Train Epoch: 4 [42000/60000] Loss: 1.546130\n",
      "Train Epoch: 4 [44000/60000] Loss: 1.539047\n",
      "Train Epoch: 4 [46000/60000] Loss: 1.558300\n",
      "Train Epoch: 4 [48000/60000] Loss: 1.523463\n",
      "Train Epoch: 4 [50000/60000] Loss: 1.556537\n",
      "Train Epoch: 4 [52000/60000] Loss: 1.512928\n",
      "Train Epoch: 4 [54000/60000] Loss: 1.559839\n",
      "Train Epoch: 4 [56000/60000] Loss: 1.517141\n",
      "Train Epoch: 4 [58000/60000] Loss: 1.502063\n",
      "\n",
      "Test set: Average loss: 0.0150, Accuracy: 9623/10000 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/60000] Loss: 1.497087\n",
      "Train Epoch: 5 [2000/60000] Loss: 1.546759\n",
      "Train Epoch: 5 [4000/60000] Loss: 1.538105\n",
      "Train Epoch: 5 [6000/60000] Loss: 1.537183\n",
      "Train Epoch: 5 [8000/60000] Loss: 1.562072\n",
      "Train Epoch: 5 [10000/60000] Loss: 1.545614\n",
      "Train Epoch: 5 [12000/60000] Loss: 1.527504\n",
      "Train Epoch: 5 [14000/60000] Loss: 1.570986\n",
      "Train Epoch: 5 [16000/60000] Loss: 1.519933\n",
      "Train Epoch: 5 [18000/60000] Loss: 1.530797\n",
      "Train Epoch: 5 [20000/60000] Loss: 1.564160\n",
      "Train Epoch: 5 [22000/60000] Loss: 1.553358\n",
      "Train Epoch: 5 [24000/60000] Loss: 1.514741\n",
      "Train Epoch: 5 [26000/60000] Loss: 1.555427\n",
      "Train Epoch: 5 [28000/60000] Loss: 1.514746\n",
      "Train Epoch: 5 [30000/60000] Loss: 1.537609\n",
      "Train Epoch: 5 [32000/60000] Loss: 1.585492\n",
      "Train Epoch: 5 [34000/60000] Loss: 1.533202\n",
      "Train Epoch: 5 [36000/60000] Loss: 1.573776\n",
      "Train Epoch: 5 [38000/60000] Loss: 1.580137\n",
      "Train Epoch: 5 [40000/60000] Loss: 1.547228\n",
      "Train Epoch: 5 [42000/60000] Loss: 1.526262\n",
      "Train Epoch: 5 [44000/60000] Loss: 1.587834\n",
      "Train Epoch: 5 [46000/60000] Loss: 1.514451\n",
      "Train Epoch: 5 [48000/60000] Loss: 1.540795\n",
      "Train Epoch: 5 [50000/60000] Loss: 1.551525\n",
      "Train Epoch: 5 [52000/60000] Loss: 1.568345\n",
      "Train Epoch: 5 [54000/60000] Loss: 1.513320\n",
      "Train Epoch: 5 [56000/60000] Loss: 1.559156\n",
      "Train Epoch: 5 [58000/60000] Loss: 1.489282\n",
      "\n",
      "Test set: Average loss: 0.0149, Accuracy: 9679/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/60000] Loss: 1.539661\n",
      "Train Epoch: 6 [2000/60000] Loss: 1.571152\n",
      "Train Epoch: 6 [4000/60000] Loss: 1.493069\n",
      "Train Epoch: 6 [6000/60000] Loss: 1.552875\n",
      "Train Epoch: 6 [8000/60000] Loss: 1.574395\n",
      "Train Epoch: 6 [10000/60000] Loss: 1.548400\n",
      "Train Epoch: 6 [12000/60000] Loss: 1.513878\n",
      "Train Epoch: 6 [14000/60000] Loss: 1.541622\n",
      "Train Epoch: 6 [16000/60000] Loss: 1.506997\n",
      "Train Epoch: 6 [18000/60000] Loss: 1.574071\n",
      "Train Epoch: 6 [20000/60000] Loss: 1.520309\n",
      "Train Epoch: 6 [22000/60000] Loss: 1.534538\n",
      "Train Epoch: 6 [24000/60000] Loss: 1.540749\n",
      "Train Epoch: 6 [26000/60000] Loss: 1.497422\n",
      "Train Epoch: 6 [28000/60000] Loss: 1.548514\n",
      "Train Epoch: 6 [30000/60000] Loss: 1.528991\n",
      "Train Epoch: 6 [32000/60000] Loss: 1.556519\n",
      "Train Epoch: 6 [34000/60000] Loss: 1.577090\n",
      "Train Epoch: 6 [36000/60000] Loss: 1.527310\n",
      "Train Epoch: 6 [38000/60000] Loss: 1.496645\n",
      "Train Epoch: 6 [40000/60000] Loss: 1.549856\n",
      "Train Epoch: 6 [42000/60000] Loss: 1.551051\n",
      "Train Epoch: 6 [44000/60000] Loss: 1.538977\n",
      "Train Epoch: 6 [46000/60000] Loss: 1.578142\n",
      "Train Epoch: 6 [48000/60000] Loss: 1.612086\n",
      "Train Epoch: 6 [50000/60000] Loss: 1.520114\n",
      "Train Epoch: 6 [52000/60000] Loss: 1.546035\n",
      "Train Epoch: 6 [54000/60000] Loss: 1.519588\n",
      "Train Epoch: 6 [56000/60000] Loss: 1.523711\n",
      "Train Epoch: 6 [58000/60000] Loss: 1.579828\n",
      "\n",
      "Test set: Average loss: 0.0149, Accuracy: 9705/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [0/60000] Loss: 1.479432\n",
      "Train Epoch: 7 [2000/60000] Loss: 1.519893\n",
      "Train Epoch: 7 [4000/60000] Loss: 1.547290\n",
      "Train Epoch: 7 [6000/60000] Loss: 1.506744\n",
      "Train Epoch: 7 [8000/60000] Loss: 1.554346\n",
      "Train Epoch: 7 [10000/60000] Loss: 1.510551\n",
      "Train Epoch: 7 [12000/60000] Loss: 1.522598\n",
      "Train Epoch: 7 [14000/60000] Loss: 1.515152\n",
      "Train Epoch: 7 [16000/60000] Loss: 1.518180\n",
      "Train Epoch: 7 [18000/60000] Loss: 1.542858\n",
      "Train Epoch: 7 [20000/60000] Loss: 1.523060\n",
      "Train Epoch: 7 [22000/60000] Loss: 1.513308\n",
      "Train Epoch: 7 [24000/60000] Loss: 1.532103\n",
      "Train Epoch: 7 [26000/60000] Loss: 1.482968\n",
      "Train Epoch: 7 [28000/60000] Loss: 1.532275\n",
      "Train Epoch: 7 [30000/60000] Loss: 1.530900\n",
      "Train Epoch: 7 [32000/60000] Loss: 1.521955\n",
      "Train Epoch: 7 [34000/60000] Loss: 1.525663\n",
      "Train Epoch: 7 [36000/60000] Loss: 1.531593\n",
      "Train Epoch: 7 [38000/60000] Loss: 1.553996\n",
      "Train Epoch: 7 [40000/60000] Loss: 1.558309\n",
      "Train Epoch: 7 [42000/60000] Loss: 1.503785\n",
      "Train Epoch: 7 [44000/60000] Loss: 1.550107\n",
      "Train Epoch: 7 [46000/60000] Loss: 1.553955\n",
      "Train Epoch: 7 [48000/60000] Loss: 1.541458\n",
      "Train Epoch: 7 [50000/60000] Loss: 1.554423\n",
      "Train Epoch: 7 [52000/60000] Loss: 1.521199\n",
      "Train Epoch: 7 [54000/60000] Loss: 1.554870\n",
      "Train Epoch: 7 [56000/60000] Loss: 1.508861\n",
      "Train Epoch: 7 [58000/60000] Loss: 1.574087\n",
      "\n",
      "Test set: Average loss: 0.0149, Accuracy: 9719/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [0/60000] Loss: 1.512185\n",
      "Train Epoch: 8 [2000/60000] Loss: 1.542410\n",
      "Train Epoch: 8 [4000/60000] Loss: 1.536501\n",
      "Train Epoch: 8 [6000/60000] Loss: 1.483078\n",
      "Train Epoch: 8 [8000/60000] Loss: 1.586182\n",
      "Train Epoch: 8 [10000/60000] Loss: 1.525203\n",
      "Train Epoch: 8 [12000/60000] Loss: 1.537969\n",
      "Train Epoch: 8 [14000/60000] Loss: 1.511454\n",
      "Train Epoch: 8 [16000/60000] Loss: 1.540715\n",
      "Train Epoch: 8 [18000/60000] Loss: 1.590891\n",
      "Train Epoch: 8 [20000/60000] Loss: 1.529912\n",
      "Train Epoch: 8 [22000/60000] Loss: 1.514982\n",
      "Train Epoch: 8 [24000/60000] Loss: 1.528802\n",
      "Train Epoch: 8 [26000/60000] Loss: 1.516237\n",
      "Train Epoch: 8 [28000/60000] Loss: 1.512894\n",
      "Train Epoch: 8 [30000/60000] Loss: 1.565657\n",
      "Train Epoch: 8 [32000/60000] Loss: 1.567422\n",
      "Train Epoch: 8 [34000/60000] Loss: 1.498963\n",
      "Train Epoch: 8 [36000/60000] Loss: 1.507377\n",
      "Train Epoch: 8 [38000/60000] Loss: 1.506220\n",
      "Train Epoch: 8 [40000/60000] Loss: 1.518871\n",
      "Train Epoch: 8 [42000/60000] Loss: 1.529999\n",
      "Train Epoch: 8 [44000/60000] Loss: 1.503986\n",
      "Train Epoch: 8 [46000/60000] Loss: 1.527855\n",
      "Train Epoch: 8 [48000/60000] Loss: 1.498385\n",
      "Train Epoch: 8 [50000/60000] Loss: 1.505415\n",
      "Train Epoch: 8 [52000/60000] Loss: 1.520233\n",
      "Train Epoch: 8 [54000/60000] Loss: 1.544270\n",
      "Train Epoch: 8 [56000/60000] Loss: 1.540214\n",
      "Train Epoch: 8 [58000/60000] Loss: 1.514763\n",
      "\n",
      "Test set: Average loss: 0.0149, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [0/60000] Loss: 1.563600\n",
      "Train Epoch: 9 [2000/60000] Loss: 1.555706\n",
      "Train Epoch: 9 [4000/60000] Loss: 1.528484\n",
      "Train Epoch: 9 [6000/60000] Loss: 1.554264\n",
      "Train Epoch: 9 [8000/60000] Loss: 1.522743\n",
      "Train Epoch: 9 [10000/60000] Loss: 1.527247\n",
      "Train Epoch: 9 [12000/60000] Loss: 1.510410\n",
      "Train Epoch: 9 [14000/60000] Loss: 1.546656\n",
      "Train Epoch: 9 [16000/60000] Loss: 1.551021\n",
      "Train Epoch: 9 [18000/60000] Loss: 1.505376\n",
      "Train Epoch: 9 [20000/60000] Loss: 1.526991\n",
      "Train Epoch: 9 [22000/60000] Loss: 1.563431\n",
      "Train Epoch: 9 [24000/60000] Loss: 1.530404\n",
      "Train Epoch: 9 [26000/60000] Loss: 1.534344\n",
      "Train Epoch: 9 [28000/60000] Loss: 1.546030\n",
      "Train Epoch: 9 [30000/60000] Loss: 1.531159\n",
      "Train Epoch: 9 [32000/60000] Loss: 1.522961\n",
      "Train Epoch: 9 [34000/60000] Loss: 1.496798\n",
      "Train Epoch: 9 [36000/60000] Loss: 1.472505\n",
      "Train Epoch: 9 [38000/60000] Loss: 1.518342\n",
      "Train Epoch: 9 [40000/60000] Loss: 1.513006\n",
      "Train Epoch: 9 [42000/60000] Loss: 1.518900\n",
      "Train Epoch: 9 [44000/60000] Loss: 1.522148\n",
      "Train Epoch: 9 [46000/60000] Loss: 1.496517\n",
      "Train Epoch: 9 [48000/60000] Loss: 1.533701\n",
      "Train Epoch: 9 [50000/60000] Loss: 1.551335\n",
      "Train Epoch: 9 [52000/60000] Loss: 1.515357\n",
      "Train Epoch: 9 [54000/60000] Loss: 1.502133\n",
      "Train Epoch: 9 [56000/60000] Loss: 1.524159\n",
      "Train Epoch: 9 [58000/60000] Loss: 1.516233\n",
      "\n",
      "Test set: Average loss: 0.0149, Accuracy: 9733/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [0/60000] Loss: 1.529966\n",
      "Train Epoch: 10 [2000/60000] Loss: 1.480343\n",
      "Train Epoch: 10 [4000/60000] Loss: 1.515209\n",
      "Train Epoch: 10 [6000/60000] Loss: 1.541727\n",
      "Train Epoch: 10 [8000/60000] Loss: 1.516093\n",
      "Train Epoch: 10 [10000/60000] Loss: 1.485566\n",
      "Train Epoch: 10 [12000/60000] Loss: 1.517917\n",
      "Train Epoch: 10 [14000/60000] Loss: 1.558248\n",
      "Train Epoch: 10 [16000/60000] Loss: 1.502235\n",
      "Train Epoch: 10 [18000/60000] Loss: 1.532435\n",
      "Train Epoch: 10 [20000/60000] Loss: 1.529763\n",
      "Train Epoch: 10 [22000/60000] Loss: 1.532870\n",
      "Train Epoch: 10 [24000/60000] Loss: 1.504560\n",
      "Train Epoch: 10 [26000/60000] Loss: 1.531694\n",
      "Train Epoch: 10 [28000/60000] Loss: 1.533195\n",
      "Train Epoch: 10 [30000/60000] Loss: 1.541180\n",
      "Train Epoch: 10 [32000/60000] Loss: 1.531500\n",
      "Train Epoch: 10 [34000/60000] Loss: 1.555162\n",
      "Train Epoch: 10 [36000/60000] Loss: 1.526195\n",
      "Train Epoch: 10 [38000/60000] Loss: 1.557886\n",
      "Train Epoch: 10 [40000/60000] Loss: 1.521847\n",
      "Train Epoch: 10 [42000/60000] Loss: 1.540714\n",
      "Train Epoch: 10 [44000/60000] Loss: 1.525538\n",
      "Train Epoch: 10 [46000/60000] Loss: 1.553660\n",
      "Train Epoch: 10 [48000/60000] Loss: 1.527017\n",
      "Train Epoch: 10 [50000/60000] Loss: 1.522745\n",
      "Train Epoch: 10 [52000/60000] Loss: 1.517846\n",
      "Train Epoch: 10 [54000/60000] Loss: 1.530106\n",
      "Train Epoch: 10 [56000/60000] Loss: 1.508172\n",
      "Train Epoch: 10 [58000/60000] Loss: 1.554405\n",
      "\n",
      "Test set: Average loss: 0.0148, Accuracy: 9772/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 11):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3dbead",
   "metadata": {},
   "source": [
    "Sprawdzam dla poszczególnego obrazu predykcje modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44066b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pilcz\\AppData\\Local\\Temp\\ipykernel_9948\\2897722950.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(X)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "data, target=test_data[5]\n",
    "data=data.unsqueeze(0).to(device)\n",
    "output=model(data)\n",
    "pred=output.argmax(dim=1, keepdim=True).item()\n",
    "print(f'Predicted: {pred}')\n",
    "image=data.squeeze(0).squeeze(0).cpu().numpy()\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
