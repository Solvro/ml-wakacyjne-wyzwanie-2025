{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "F_stIEd42YVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sieci Neuronowe\n"
      ],
      "metadata": {
        "id": "SiXGzMmw-kM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Celem tego notebooka bƒôdzie przedstawienie najwazniejszych funkcji Pytorcha, kt√≥re przydadzƒÖ siƒô wam zar√≥wno do wykonania zadania jak i do pracy z tƒÖ bibliotekƒÖ w przysz≈Ço≈õci. </br>\n",
        "Pytorch to open-sourcowa biblioteka Pythonowa kt√≥ra na poczƒÖtku funkcjonowa≈Ça jako wewnƒôtrzne narzƒôdzie firmy Meta, kt√≥rƒÖ postanowiono upubliczniƒá w celu rozpromowania i u≈Çatwienia researchu nad uczeniem g≈Çƒôbokim (oraz ≈ºeby zrobiƒá konkurencje dla Google'owego TensorFlow).\n",
        "\n",
        "\n",
        "\n",
        "Ekosystem PyTorcha sk≈Çada siƒô z kilku modu≈Ç√≥w:\n",
        "\n",
        "\n",
        "*   `torch` ‚Äì podstawowe operacje na tensorach.\n",
        "*   `torch.nn` ‚Äì modu≈Çy do budowy sieci neuronowych.\n",
        "*   `torch.optim` ‚Äì algorytmy optymalizacji.\n",
        "*   `torch.utils.data` ‚Äì efektywne ≈Çadowanie i przetwarzanie danych.\n",
        "*   `torchvision` - narzƒôdzia do widzenia komputerowego\n",
        "\n",
        "Oraz kilka innych element√≥w jak PyTorch lightning u≈ÇatwiajƒÖcych tworzenie wszelkiego rodzaji algorytm√≥w sztucznej inteligencji. </br> </br>\n",
        "Absolutna podstawƒÖ pracy w PyTorchu sƒÖ tensory - odpowiedniki `np.array` z NumPy, przechowujƒÖce dane. NajwiƒôkszƒÖ zaletƒÖ tensor√≥w jest mo≈ºliwo≈õƒá wykonywania oblicze≈Ñ na nich na GPU, dlaczego jest to takie u≈ºyteczne? Poka≈ºemy to w p√≥≈∫niejszej czƒô≈õci notatnika je≈õli kto≈õ nie pamiƒôta z wyk≈Çadu üôÇ\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HZE4eVcR_XV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Wstƒôp - PyTorch"
      ],
      "metadata": {
        "id": "NM52HO-Y-pwU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Najpro≈õciej my≈õleƒá o tensorach jako o macierzach o dowolnej ilo≈õci wymiar√≥w.\n",
        "\n",
        "\n",
        "*   Tensor 0-wymiarowy - skalar\n",
        "*   Tensor 1-wymiarowy - wektor\n",
        "*   Tensor 2-wymiarowy - macierz\n",
        "*   Tensor 3-lub-wiƒôcej-wymiarowy - nie znam nazwy ale najczƒô≈õciej takie tensory reprezentujƒÖ zdjƒôcia (wysoko≈õƒá, szeroko≈õƒá, kana≈Çy RGB) lub inne wielowymiarowe dane\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VKwjyDqnIn16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# przyk≈Çady tensor√≥w\n",
        "skalar = torch.tensor(1)\n",
        "\n",
        "wektor = torch.tensor([1,2,3])\n",
        "\n",
        "macierz = torch.tensor([[1,2],[3,4]])\n",
        "\n",
        "print(skalar,'\\n', wektor,'\\n', macierz)"
      ],
      "metadata": {
        "id": "MXwjxAnQ4bRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensory mo≈ºemy tworzyƒá na wiele sposob√≥w, przerzucajƒÖc je z array od NumPy, wype≈ÇniajƒÖc macierze zerami czy jedynkami (w zalezno≈õci od potrzeby), czy przydzielajac losowe warto≈õci. Indeksowanie tensor√≥w jest analogiczne do tego jak dzia≈Ça ono w NumPy."
      ],
      "metadata": {
        "id": "TRSpyRP7KczG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Przyk≈Çadowy array\n",
        "dane = [[1,2],[3,4]]\n",
        "x_dane = np.array(dane)\n",
        "\n",
        "#Zmiana na tensory\n",
        "x_tensor = torch.from_numpy(x_dane)\n",
        "\n",
        "#Tworzenie sztucznych macierzy\n",
        "zera = torch.zeros(2,3)\n",
        "jedynki = torch.ones(2,3)\n",
        "losowe_liczby = torch.rand(2,3)\n",
        "\n",
        "print(zera,'\\n', jedynki, '\\n',losowe_liczby)"
      ],
      "metadata": {
        "id": "m2aZvZm-K83a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losowe_liczby.shape"
      ],
      "metadata": {
        "id": "Mz0TiNJONwCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    x_tensor = x_tensor.to('cuda')\n"
      ],
      "metadata": {
        "id": "cZxKLYrAMVNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([3.0], requires_grad=True)\n",
        "b = torch.tensor([5.0], requires_grad=True)\n",
        "Q = 3 * a**3 - b**2\n",
        "Q.backward()  # oblicza gradienty\n",
        "print(a.grad) # dQ/da\n",
        "print(b.grad) # dQ/db\n"
      ],
      "metadata": {
        "id": "qSbusKc9N50B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24d786c9"
      },
      "source": [
        "### Indeksowanie i Wycinanie Tensor√≥w\n",
        "\n",
        "Podobnie jak w NumPy, mo≈ºemy u≈ºywaƒá indeksowania i wycinania do dostƒôpu do element√≥w lub podzbior√≥w tensor√≥w."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e41bf1a"
      },
      "source": [
        "tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "\n",
        "# Dostƒôp do pojedynczego elementu\n",
        "print(f\"Pojedynczy element: {tensor[0, 0]}\\n\")\n",
        "\n",
        "# Wyciƒôcie wiersza\n",
        "print(f\"Wyciƒôcie wiersza: {tensor[1, :]}\\n\")\n",
        "\n",
        "# Wyciƒôcie kolumny\n",
        "print(f\"Wyciƒôcie kolumny: {tensor[:, 2]}\\n\")\n",
        "\n",
        "# Wyciƒôcie podzbioru\n",
        "print(f\"Wyciƒôcie podzbioru:\\n {tensor[0:2, 1:3]}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed0c7ee1"
      },
      "source": [
        "### Zmiana Kszta≈Çtu Tensor√≥w (Reshaping/Viewing)\n",
        "\n",
        "Czƒôsto potrzebujemy zmieniƒá kszta≈Çt tensora bez zmiany jego danych. Mo≈ºemy to zrobiƒá za pomocƒÖ metod `reshape` lub `view`. Metoda `view` dzia≈Ça tylko wtedy, gdy tensor jest ciƒÖg≈Çy w pamiƒôci."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a51b8e0"
      },
      "source": [
        "tensor = torch.arange(9).reshape(3, 3)\n",
        "print(f\"Oryginalny tensor:\\n {tensor}\\n\")\n",
        "\n",
        "# Zmiana kszta≈Çtu na wektor\n",
        "wektor_tensor = tensor.reshape(-1)\n",
        "print(f\"Tensor zmieniony na wektor: {wektor_tensor}\\n\")\n",
        "\n",
        "# Zmiana kszta≈Çtu na inny rozmiar\n",
        "nowy_ksztalt_tensor = tensor.reshape(9, 1)\n",
        "print(f\"Tensor o nowym kszta≈Çcie:\\n {nowy_ksztalt_tensor}\\n\")\n",
        "\n",
        "# U≈ºycie .view() (je≈õli mo≈ºliwe)\n",
        "widok_tensor = tensor.view(9)\n",
        "print(f\"Widok tensora: {widok_tensor}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04029e7b"
      },
      "source": [
        "### ≈ÅƒÖczenie (Concatenating) i Stackowanie (Stacking) Tensor√≥w\n",
        "\n",
        "Tensory mo≈ºna ≈ÇƒÖczyƒá wzd≈Çu≈º istniejƒÖcego wymiaru za pomocƒÖ `torch.cat` lub tworzyƒá nowy wymiar za pomocƒÖ `torch.stack`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39d0b70c"
      },
      "source": [
        "tensor1 = torch.tensor([[1, 2], [3, 4]])\n",
        "tensor2 = torch.tensor([[5, 6], [7, 8]])\n",
        "\n",
        "# ≈ÅƒÖczenie wzd≈Çu≈º wierszy (wymiar 0)\n",
        "concatenated_rows = torch.cat([tensor1, tensor2], dim=0)\n",
        "print(f\"≈ÅƒÖczenie wzd≈Çu≈º wierszy:\\n {concatenated_rows}\\n\")\n",
        "\n",
        "# ≈ÅƒÖczenie wzd≈Çu≈º kolumn (wymiar 1)\n",
        "concatenated_cols = torch.cat([tensor1, tensor2], dim=1)\n",
        "print(f\"≈ÅƒÖczenie wzd≈Çu≈º kolumn:\\n {concatenated_cols}\\n\")\n",
        "\n",
        "# Stackowanie (tworzy nowy wymiar na poczƒÖtku domy≈õlnie)\n",
        "stacked_tensors = torch.stack([tensor1, tensor2])\n",
        "print(f\"Stackowanie:\\n {stacked_tensors}\\n\")\n",
        "\n",
        "# Stackowanie wzd≈Çu≈º innego wymiaru\n",
        "stacked_tensors_dim1 = torch.stack([tensor1, tensor2], dim=1)\n",
        "print(f\"Stackowanie wzd≈Çu≈º wymiaru 1:\\n {stacked_tensors_dim1}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Sieci Neuronowe"
      ],
      "metadata": {
        "id": "bfKp2cdl-wJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modu≈Ç `torch.nn` jest podstawowym narzƒôdziem w PyTorchu do budowania i trenowania sieci neuronowych. Zamiast implementowaƒá ka≈ºdy element sieci od podstaw, `torch.nn` dostarcza bogaty zestaw gotowych blok√≥w konstrukcyjnych (ang. building blocks). Modu≈Ç ten jest kluczowy, poniewa≈º zawiera definicje standardowych warstw neuronowych (takich jak warstwy liniowe, konwolucyjne), funkcji aktywacji (np. ReLU, Sigmoid), funkcji straty (np. MSELoss, CrossEntropyLoss) oraz narzƒôdzia do zarzƒÖdzania parametrami modelu i gradientami. Umo≈ºliwia to szybsze i bardziej efektywne tworzenie z≈Ço≈ºonych architektur sieci neuronowych."
      ],
      "metadata": {
        "id": "Vso4vstZLuMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Warstwy\n",
        "\n",
        "Modu≈Ç `torch.nn` oferuje wiele predefiniowanych warstw, kt√≥re stanowiƒÖ podstawowe elementy budulcowe sieci neuronowych. Dwie z najczƒô≈õciej u≈ºywanych warstw to `Linear` i `Conv2d`.\n",
        "\n",
        "Warstwa `Linear` (nazywana r√≥wnie≈º warstwƒÖ w pe≈Çni po≈ÇƒÖczonƒÖ lub gƒôstƒÖ) wykonuje transformacjƒô liniowƒÖ danych wej≈õciowych: $y = xA^T + b$. Jest powszechnie u≈ºywana w ko≈Ñcowych warstwach sieci do mapowania cech na wyniki klasyfikacji lub regresji, a tak≈ºe jako warstwa ukryta w prostszych sieciach.\n",
        "\n",
        "Warstwa `Conv2d` (warstwa konwolucyjna 2D) jest fundamentalna w sieciach neuronowych do przetwarzania danych o strukturze siatki, takich jak obrazy. Wykonuje operacjƒô splotu, uczƒÖc siƒô filtr√≥w, kt√≥re wykrywajƒÖ wzorce przestrzenne w danych wej≈õciowych (np. krawƒôdzie, tekstury). Jest kluczowa w wiƒôkszo≈õci nowoczesnych architektur do widzenia komputerowego."
      ],
      "metadata": {
        "id": "SC9iubO1L7Q1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7db7c12b"
      },
      "source": [
        "# Przyk≈Çad u≈ºycia warstwy Linear\n",
        "input_features = 10\n",
        "output_features = 5\n",
        "batch_size = 64\n",
        "\n",
        "# Tworzenie przyk≈Çadowego tensora wej≈õciowego (partia x cechy wej≈õciowe)\n",
        "linear_input = torch.randn(batch_size, input_features)\n",
        "\n",
        "# Definiowanie warstwy Linear\n",
        "linear_layer = torch.nn.Linear(input_features, output_features)\n",
        "\n",
        "# Przepuszczenie tensora przez warstwƒô\n",
        "linear_output = linear_layer(linear_input)\n",
        "\n",
        "print(f\"Kszta≈Çt tensora wej≈õciowego dla warstwy Linear: {linear_input.shape}\")\n",
        "print(f\"Kszta≈Çt tensora wyj≈õciowego z warstwy Linear: {linear_output.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b169195"
      },
      "source": [
        "### Funkcje aktywacji (activation functions)\n",
        "\n",
        "\n",
        "Funkcje aktywacji odgrywajƒÖ kluczowƒÖ rolƒô w sieciach neuronowych, wprowadzajƒÖc nieliniowo≈õƒá do modelu. Bez funkcji aktywacji, sieƒá neuronowa sk≈ÇadajƒÖca siƒô z wielu warstw liniowych by≈Çaby r√≥wnowa≈ºna pojedynczej warstwie liniowej, co znacznie ograniczy≈Çoby jej zdolno≈õƒá do modelowania z≈Ço≈ºonych zale≈ºno≈õci w danych.\n",
        "\n",
        "Popularne funkcje aktywacji to:\n",
        "\n",
        "*   **ReLU (Rectified Linear Unit):** $f(x) = \\max(0, x)$. Jest to najczƒô≈õciej u≈ºywana funkcja aktywacji ze wzglƒôdu na swojƒÖ prostotƒô i efektywno≈õƒá obliczeniowƒÖ. Pomaga w rozwiƒÖzywaniu problemu zanikajƒÖcych gradient√≥w.\n",
        "*   **Sigmoid:** $f(x) = \\frac{1}{1 + e^{-x}}$. Funkcja Sigmoid kompresuje warto≈õci wej≈õciowe do zakresu (0, 1). By≈Ça popularna w przesz≈Ço≈õci, ale obecnie jest rzadziej u≈ºywana w ukrytych warstwach ze wzglƒôdu na problem zanikajƒÖcych gradient√≥w dla du≈ºych lub ma≈Çych warto≈õci wej≈õciowych. Czƒôsto u≈ºywana w warstwie wyj≈õciowej do zada≈Ñ klasyfikacji binarnej.\n",
        "*   **Softmax:** Stosowana g≈Ç√≥wnie w warstwie wyj≈õciowej do zada≈Ñ klasyfikacji wieloklasowej. Przekszta≈Çca wektor warto≈õci rzeczywistych na rozk≈Çad prawdopodobie≈Ñstwa, gdzie suma wszystkich element√≥w wynosi 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50a47d86"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Przyk≈Çadowy tensor wej≈õciowy\n",
        "input_tensor = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
        "\n",
        "# U≈ºycie funkcji aktywacji\n",
        "relu_output = F.relu(input_tensor)\n",
        "sigmoid_output = torch.sigmoid(input_tensor) # Mo≈ºna te≈º u≈ºyƒá torch.nn.functional.sigmoid\n",
        "softmax_output = F.softmax(input_tensor, dim=0) # dim=0 poniewa≈º tensor jest 1D\n",
        "\n",
        "print(f\"Tensor wej≈õciowy: {input_tensor}\")\n",
        "print(f\"Wynik po ReLU: {relu_output}\")\n",
        "print(f\"Wynik po Sigmoid: {sigmoid_output}\")\n",
        "print(f\"Wynik po Softmax: {softmax_output}\")\n",
        "\n",
        "# Przyk≈Çad Softmax na tensorze 2D (np. wyniki z warstwy liniowej dla partii danych)\n",
        "input_2d = torch.tensor([[-1.0, 0.0, 1.0], [0.0, 1.0, 2.0]])\n",
        "softmax_output_2d = F.softmax(input_2d, dim=1) # dim=1 dla ka≈ºdej pr√≥bki w partii\n",
        "\n",
        "print(f\"\\nTensor wej≈õciowy 2D:\\n {input_2d}\")\n",
        "print(f\"Wynik po Softmax (dim=1):\\n {softmax_output_2d}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70079424"
      },
      "source": [
        "### Funkcje straty (loss functions)\n",
        "\n",
        "Funkcje straty sƒÖ kluczowym elementem procesu treningu sieci neuronowych. MierzƒÖ one r√≥≈ºnicƒô miƒôdzy przewidywaniami modelu a rzeczywistymi warto≈õciami docelowymi. Celem treningu jest minimalizacja tej funkcji straty, co oznacza, ≈ºe model uczy siƒô generowaƒá przewidywania coraz bli≈ºsze rzeczywistym warto≈õciom.\n",
        "\n",
        "W zale≈ºno≈õci od rodzaju zadania (np. regresja, klasyfikacja), stosuje siƒô r√≥≈ºne funkcje straty:\n",
        "\n",
        "*   **MSE (Mean Squared Error):** Stosowana g≈Ç√≥wnie w zadaniach **regresji**. Oblicza ≈õredni kwadrat r√≥≈ºnicy miƒôdzy przewidywanymi warto≈õciami a warto≈õciami rzeczywistymi. Im mniejsza warto≈õƒá MSE, tym lepsze dopasowanie modelu do danych. Wz√≥r: $L = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$, gdzie $y_i$ to warto≈õƒá rzeczywista, a $\\hat{y}_i$ to warto≈õƒá przewidywana.\n",
        "*   **CrossEntropyLoss:** Stosowana g≈Ç√≥wnie w zadaniach **klasyfikacji**, zw≈Çaszcza wieloklasowej. Mierzy r√≥≈ºnicƒô miƒôdzy rozk≈Çadem prawdopodobie≈Ñstwa przewidywanym przez model a rzeczywistym rozk≈Çadem (reprezentowanym przez etykietƒô klasy). W PyTorchu `CrossEntropyLoss` jest wygodnƒÖ kombinacjƒÖ funkcji Softmax (kt√≥ra przekszta≈Çca surowe wyniki modelu - logity - na prawdopodobie≈Ñstwa) i Negative Log Likelihood Loss. Oczekuje logit√≥w jako wej≈õcia i etykiet klas (indeks√≥w) jako celu.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c65a03a9"
      },
      "source": [
        "\n",
        "# Przyk≈Çad u≈ºycia MSELoss (dla regresji)\n",
        "\n",
        "# Przyk≈Çadowe przewidywane warto≈õci (np. wyj≈õcie modelu regresyjnego)\n",
        "predictions = torch.tensor([1.5, 2.3, 3.1, 4.9, 5.2])\n",
        "\n",
        "# Przyk≈Çadowe rzeczywiste warto≈õci (np. prawdziwe warto≈õci docelowe)\n",
        "targets = torch.tensor([1.0, 2.5, 3.0, 5.0, 5.5])\n",
        "\n",
        "# Definiowanie funkcji straty MSE\n",
        "mse_loss = nn.MSELoss()\n",
        "\n",
        "# Obliczanie straty\n",
        "loss = mse_loss(predictions, targets)\n",
        "\n",
        "print(f\"Przewidywane warto≈õci: {predictions}\")\n",
        "print(f\"Rzeczywiste warto≈õci: {targets}\")\n",
        "print(f\"Warto≈õƒá straty MSE: {loss.item()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "061ba518"
      },
      "source": [
        "\n",
        "# Przyk≈Çad u≈ºycia CrossEntropyLoss (dla klasyfikacji)\n",
        "\n",
        "# Przyk≈Çadowe logity (surowe wyj≈õcie sieci neuronowej przed funkcjƒÖ Softmax)\n",
        "# Wymiary: partia x liczba klas\n",
        "logits = torch.tensor([[-0.1, 0.5, -0.3], [1.2, -0.8, 0.1], [-0.5, -0.2, 1.0]])\n",
        "\n",
        "# Przyk≈Çadowe etykiety klas (indeksy prawdziwych klas dla ka≈ºdej pr√≥bki w partii)\n",
        "# Wymiary: partia\n",
        "# Etykiety powinny byƒá torch.long\n",
        "labels = torch.tensor([1, 0, 2], dtype=torch.long)\n",
        "\n",
        "# Definiowanie funkcji straty Cross Entropy\n",
        "ce_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# Obliczanie straty\n",
        "loss = ce_loss(logits, labels)\n",
        "\n",
        "print(f\"Logity (wyj≈õcie modelu):\")\n",
        "print(logits)\n",
        "print(f\"Etykiety klas:\")\n",
        "print(labels)\n",
        "print(f\"Warto≈õƒá straty Cross Entropy: {loss.item()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e52816bc"
      },
      "source": [
        "### Budowa prostej sieci\n",
        "\n",
        "W PyTorchu, standardowym sposobem definiowania w≈Çasnych architektur sieci neuronowych jest dziedziczenie po klasie bazowej `torch.nn.Module`. Klasa ta zapewnia funkcjonalno≈õƒá potrzebnƒÖ do ≈õledzenia parametr√≥w modelu (wag i bias√≥w), zarzƒÖdzania nimi na r√≥≈ºnych urzƒÖdzeniach (CPU/GPU) oraz automatycznego obliczania gradient√≥w podczas wstecznej propagacji.\n",
        "\n",
        "Aby zbudowaƒá w≈ÇasnƒÖ sieƒá neuronowƒÖ, nale≈ºy:\n",
        "\n",
        "1.  **Zdefiniowaƒá klasƒô**, kt√≥ra dziedziczy po `torch.nn.Module`.\n",
        "2.  W metodzie `__init__` (konstruktorze klasy), **zdefiniowaƒá warstwy i inne komponenty** (np. funkcje aktywacji, choƒá czƒôsto sƒÖ one u≈ºywane bezpo≈õrednio w metodzie `forward`), kt√≥re bƒôdƒÖ u≈ºywane w sieci. Nale≈ºy zarejestrowaƒá te warstwy jako atrybuty klasy, aby `torch.nn.Module` m√≥g≈Ç poprawnie zarzƒÖdzaƒá ich parametrami.\n",
        "3.  W metodzie `forward`, **zdefiniowaƒá przep≈Çyw danych** przez sieƒá. Metoda ta przyjmuje tensor wej≈õciowy i zwraca tensor wyj≈õciowy po przepuszczeniu go przez zdefiniowane warstwy i funkcje aktywacji w odpowiedniej kolejno≈õci. To w≈Ça≈õnie w tej metodzie realizowana jest logika obliczeniowa sieci.\n",
        "\n",
        "Poni≈ºej przedstawiamy przyk≈Çad prostej sieci neuronowej sk≈ÇadajƒÖcej siƒô z kilku warstw liniowych i funkcji aktywacji ReLU.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b98dd24"
      },
      "source": [
        "# 1. Zdefiniuj prostƒÖ klasƒô sieci neuronowej\n",
        "class SimpleNN(nn.Module):\n",
        "    # 2. Zdefiniuj warstwy i funkcje aktywacji w __init__\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size) # Pierwsza warstwa liniowa\n",
        "        self.relu = nn.ReLU()                       # Funkcja aktywacji ReLU\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size) # Druga warstwa liniowa\n",
        "\n",
        "    # 3. Zaimplementuj przep≈Çyw danych w forward\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)    # Przepu≈õƒá dane przez pierwszƒÖ warstwƒô liniowƒÖ\n",
        "        x = self.relu(x)   # Zastosuj funkcjƒô aktywacji ReLU\n",
        "        x = self.fc2(x)    # Przepu≈õƒá dane przez drugƒÖ warstwƒô liniowƒÖ\n",
        "        return x\n",
        "\n",
        "# 4. Utw√≥rz instancjƒô zdefiniowanej klasy sieci neuronowej\n",
        "input_dim = 10\n",
        "hidden_dim = 20\n",
        "output_dim = 5\n",
        "model = SimpleNN(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# 5. Stw√≥rz przyk≈Çadowy tensor wej≈õciowy\n",
        "# Przyk≈Çad: partia 64 pr√≥bek, ka≈ºda z 10 cechami\n",
        "sample_input = torch.randn(64, input_dim)\n",
        "\n",
        "# 6. Przepu≈õƒá przyk≈Çadowy tensor wej≈õciowy przez zdefiniowanƒÖ sieƒá neuronowƒÖ\n",
        "output_tensor = model(sample_input) # Wywo≈Çanie instancji klasy uruchamia metodƒô forward\n",
        "\n",
        "# 7. Wypisz kszta≈Çt tensora wyj≈õciowego\n",
        "print(f\"Kszta≈Çt wej≈õciowego tensora: {sample_input.shape}\")\n",
        "print(f\"Kszta≈Çt wyj≈õciowego tensora z sieci: {output_tensor.shape}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35e5510e"
      },
      "source": [
        "### Trening modelu (podstawy)\n",
        "\n",
        "Trening sieci neuronowej w PyTorchu zazwyczaj obejmuje nastƒôpujƒÖce podstawowe kroki, wykonywane iteracyjnie dla ka≈ºdej partii danych:\n",
        "\n",
        "1.  **Forward pass:** W tym kroku dane wej≈õciowe sƒÖ przepuszczane przez sieƒá neuronowƒÖ, aby wygenerowaƒá przewidywania modelu (wyj≈õcie). Obliczenia przep≈ÇywajƒÖ od warstwy wej≈õciowej do warstwy wyj≈õciowej.\n",
        "2.  **Loss calculation:** Po uzyskaniu przewidywa≈Ñ, obliczana jest warto≈õƒá funkcji straty (np. `MSELoss` dla regresji, `CrossEntropyLoss` dla klasyfikacji). Funkcja straty kwantyfikuje, jak bardzo przewidywania modelu r√≥≈ºniƒÖ siƒô od rzeczywistych etykiet docelowych.\n",
        "3.  **Backward pass:** To kluczowy krok, w kt√≥rym PyTorch oblicza gradienty funkcji straty wzglƒôdem wszystkich parametr√≥w modelu (wag i bias√≥w). Dzieje siƒô to dziƒôki mechanizmowi automatycznego r√≥≈ºniczkowania PyTorcha. Wywo≈Çanie metody `.backward()` na tensorze reprezentujƒÖcym warto≈õƒá funkcji straty inicjuje ten proces. Gradienty wskazujƒÖ kierunek i si≈Çƒô, w jakiej nale≈ºy zmieniƒá parametry, aby zminimalizowaƒá stratƒô.\n",
        "4.  **Optimizer step:** Na koniec, wybrany algorytm optymalizacji (np. SGD, Adam) wykorzystuje obliczone gradienty do aktualizacji parametr√≥w modelu. Celem optymalizatora jest znalezienie zbioru parametr√≥w, kt√≥ry minimalizuje funkcjƒô straty na danych treningowych. Metoda `optimizer.step()` wykonuje tƒô aktualizacjƒô.\n",
        "\n",
        "Rola optymalizatora i metody `.backward()` jest fundamentalna. Metoda `.backward()` dostarcza niezbƒôdnych informacji (gradient√≥w) o tym, jak zmieniƒá parametry, podczas gdy optymalizator wykorzystuje te informacje do faktycznej modyfikacji parametr√≥w w celu stopniowego poprawiania wydajno≈õci modelu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67036f97"
      },
      "source": [
        "### Tworzenie dataset√≥w i dataloader√≥w\n",
        "\n",
        "Efektywne ≈Çadowanie i przetwarzanie danych jest kluczowe podczas trenowania modeli uczenia g≈Çƒôbokiego, zw≈Çaszcza przy du≈ºych zbiorach danych. PyTorch dostarcza dwa podstawowe narzƒôdzia do tego celu: `torch.utils.data.Dataset` i `torch.utils.data.DataLoader`.\n",
        "\n",
        "### Klasa `Dataset`\n",
        "\n",
        "Klasa `Dataset` reprezentuje zbi√≥r danych. W≈Çasna klasa `Dataset` powinna dziedziczyƒá po `torch.utils.data.Dataset` i implementowaƒá dwie kluczowe metody:\n",
        "\n",
        "*   `__len__(self)`: Powinna zwracaƒá ca≈Çkowity rozmiar zbioru danych.\n",
        "*   `__getitem__(self, idx)`: Powinna zwracaƒá pojedynczƒÖ pr√≥bkƒô danych (np. para cechy-etykieta) dla danego indeksu `idx`.\n",
        "\n",
        "ImplementujƒÖc te dwie metody, tworzymy obiekt, kt√≥ry mo≈ºna indeksowaƒá jak listƒô (`dataset[i]`) i kt√≥ry zna sw√≥j rozmiar (`len(dataset)`). Klasa `Dataset` odpowiada za logikƒô pobierania pojedynczych pr√≥bek danych z ich ≈∫r√≥d≈Ça (np. pliki na dysku, baza danych).\n",
        "\n",
        "### Klasa `DataLoader`\n",
        "\n",
        "Klasa `DataLoader` jest iteratorem, kt√≥ry opakowuje obiekt `Dataset` i zapewnia wygodny spos√≥b na dostƒôp do danych w partiach (mini-batchach). Automatyzuje takie procesy jak:\n",
        "\n",
        "*   **Partiowanie (Batching):** Grupowanie pojedynczych pr√≥bek w partie.\n",
        "*   **Mieszanie (Shuffling):** Opcjonalne mieszanie danych w ka≈ºdej epoce, co jest wa≈ºne dla stabilno≈õci treningu.\n",
        "*   **R√≥wnoleg≈Çe ≈Çadowanie (Parallel loading):** ≈Åadowanie danych w wielu procesach roboczych, aby przyspieszyƒá proces I/O i zapobiec wƒÖskiemu gard≈Çu w treningu.\n",
        "\n",
        "Kluczowe argumenty `DataLoader` to:\n",
        "\n",
        "*   `dataset`: Obiekt klasy `Dataset`, z kt√≥rego bƒôdƒÖ pobierane dane.\n",
        "*   `batch_size`: Liczba pr√≥bek w ka≈ºdej partii.\n",
        "*   `shuffle`: Je≈õli `True`, dane bƒôdƒÖ mieszane w ka≈ºdej epoce.\n",
        "\n",
        "### Customowa funkcja `collate_fn`\n",
        "\n",
        "Domy≈õlnie `DataLoader` pr√≥buje automatycznie ≈ÇƒÖczyƒá pr√≥bki w partiƒô. Dzia≈Ça to dobrze, gdy pr√≥bki majƒÖ identyczny kszta≈Çt (np. obrazy o tym samym rozmiarze). Jednak w przypadku danych o r√≥≈ºnych rozmiarach (np. sekwencje tekstowe o r√≥≈ºnej d≈Çugo≈õci, grafy o r√≥≈ºnej liczbie wƒôz≈Ç√≥w), standardowy mechanizm ≈ÇƒÖczenia mo≈ºe zawie≈õƒá.\n",
        "\n",
        "W takich sytuacjach mo≈ºna zdefiniowaƒá w≈ÇasnƒÖ funkcjƒô `collate_fn` i przekazaƒá jƒÖ do `DataLoader`. Ta funkcja przyjmuje listƒô pr√≥bek zwr√≥conych przez `dataset.__getitem__` dla danej partii i jest odpowiedzialna za ich odpowiednie przetworzenie i po≈ÇƒÖczenie w jeden lub wiƒôcej tensor√≥w, kt√≥re bƒôdƒÖ reprezentowaƒá partiƒô danych. Przyk≈Çadowo, `collate_fn` mo≈ºe dodawaƒá wype≈Çnienie (padding) do sekwencji o r√≥≈ºnej d≈Çugo≈õci, aby mia≈Çy ten sam rozmiar przed po≈ÇƒÖczeniem w tensor.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fedc52f"
      },
      "source": [
        "\n",
        "\n",
        "class SimpleCustomDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        # Sprawdzenie, czy d≈Çugo≈õci danych i etykiet sƒÖ zgodne\n",
        "        if len(data) != len(labels):\n",
        "            raise ValueError(\"Data and labels must have the same length\")\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        # Zwraca ca≈Çkowity rozmiar zbioru danych\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Zwraca pojedynczƒÖ pr√≥bkƒô danych i jej etykietƒô dla danego indeksu\n",
        "        # Zak≈Çadamy, ≈ºe dane i etykiety sƒÖ ju≈º tensorami lub listami, kt√≥re mo≈ºna ≈Çatwo przekonwertowaƒá\n",
        "        sample = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "        # Konwersja na tensory PyTorch (je≈õli nie sƒÖ ju≈º tensorami)\n",
        "        if not isinstance(sample, torch.Tensor):\n",
        "            sample = torch.tensor(sample, dtype=torch.float32)\n",
        "        if not isinstance(label, torch.Tensor):\n",
        "            label = torch.tensor(label, dtype=torch.long) # Etykiety klas zazwyczaj sƒÖ typu long\n",
        "\n",
        "        return sample, label\n",
        "\n",
        "# Przyk≈Çadowe dane i etykiety\n",
        "sample_data = [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0], [7.0, 8.0], [9.0, 10.0], [11.0, 12.0]]\n",
        "sample_labels = [0, 1, 0, 1, 0, 1]\n",
        "\n",
        "# Utworzenie instancji customowej klasy Dataset\n",
        "custom_dataset = SimpleCustomDataset(sample_data, sample_labels)\n",
        "\n",
        "# Demonstracja dostƒôpu do element√≥w i rozmiaru\n",
        "print(f\"Rozmiar zbioru danych: {len(custom_dataset)}\")\n",
        "print(f\"Pierwsza pr√≥bka: {custom_dataset[0]}\")\n",
        "print(f\"Trzecia pr√≥bka: {custom_dataset[2]}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b930c03e"
      },
      "source": [
        "# U≈ºycie DataLoader z customowym zbiorem danych\n",
        "batch_size = 2\n",
        "dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "print(f\"\\nDemonstracja iteracji po DataLoaderze z batch_size={batch_size}:\")\n",
        "# Iteracja po DataLoaderze\n",
        "for i, (batch_samples, batch_labels) in enumerate(dataloader):\n",
        "    print(f\"Numer partii: {i+1}\")\n",
        "    print(f\"Kszta≈Çt partii danych: {batch_samples.shape}\")\n",
        "    print(f\"Kszta≈Çt partii etykiet: {batch_labels.shape}\")\n",
        "    print(f\"Przyk≈Çadowa partia danych:\\n{batch_samples}\")\n",
        "    print(f\"Przyk≈Çadowa partia etykiet:\\n{batch_labels}\")\n",
        "    if i == 2: # Wypiszmy tylko kilka partii dla przyk≈Çadu\n",
        "        break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd94e27d"
      },
      "source": [
        "def custom_collate(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function to demonstrate combining samples into a batch.\n",
        "    This function simply stacks the tensors for data and labels.\n",
        "    In a real-world scenario, this would handle variable-sized data (e.g., padding).\n",
        "\n",
        "    Args:\n",
        "        batch (list): A list of samples returned by the Dataset's __getitem__.\n",
        "                      Each sample is a tuple (data, label).\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing stacked data tensors and stacked label tensors.\n",
        "               (batch_data, batch_labels)\n",
        "    \"\"\"\n",
        "    # batch is a list of tuples: [(sample1, label1), (sample2, label2), ...]\n",
        "    # Separate data and labels into two lists\n",
        "    data_list = [item[0] for item in batch]\n",
        "    label_list = [item[1] for item in batch]\n",
        "\n",
        "    # Stack the tensors along a new dimension (usually the batch dimension)\n",
        "    # torch.stack creates a new dimension at dim=0 by default\n",
        "    batch_data = torch.stack(data_list)\n",
        "    batch_labels = torch.stack(label_list)\n",
        "\n",
        "    return batch_data, batch_labels\n",
        "\n",
        "print(\"Zdefiniowano customowƒÖ funkcjƒô collate_fn.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VuqrWPhK2qI"
      },
      "source": [
        "# U≈ºycie DataLoader z customowƒÖ funkcjƒÖ collate_fn\n",
        "batch_size = 3 # U≈ºyjmy innego rozmiaru partii\n",
        "dataloader_with_collate = DataLoader(custom_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate) # shuffle=False dla powtarzalno≈õci\n",
        "\n",
        "print(f\"\\nDemonstracja iteracji po DataLoaderze z customowƒÖ funkcjƒÖ collate_fn i batch_size={batch_size}:\")\n",
        "# Iteracja po DataLoaderze z custom collate_fn\n",
        "for i, (batch_samples, batch_labels) in enumerate(dataloader_with_collate):\n",
        "    print(f\"Numer partii: {i+1}\")\n",
        "    print(f\"Kszta≈Çt partii danych: {batch_samples.shape}\")\n",
        "    print(f\"Kszta≈Çt partii etykiet: {batch_labels.shape}\")\n",
        "    print(f\"Przyk≈Çadowa partia danych:\\n{batch_samples}\")\n",
        "    print(f\"Przyk≈Çadowa partia etykiet:\\n{batch_labels}\")\n",
        "    if i == 1: # Wypiszmy tylko kilka partii dla przyk≈Çadu\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "878921e2"
      },
      "source": [
        "### Zapisywanie i wczytywanie modeli\n",
        "\n",
        "\n",
        "Po wytrenowaniu modelu neuronowego, kluczowe jest mo≈ºliwo≈õƒá jego zapisania na dysku, aby mo≈ºna go by≈Ço p√≥≈∫niej wczytaƒá i ponownie u≈ºyƒá (np. do wnioskowania na nowych danych, dalszego treningu lub udostƒôpnienia innym). PyTorch oferuje elastyczne mechanizmy do zapisywania i wczytywania modeli.\n",
        "\n",
        "IstniejƒÖ dwie g≈Ç√≥wne metody zapisywania modeli w PyTorchu:\n",
        "\n",
        "1.  **Zapisywanie samego s≈Çownika stanu (state_dict):** Jest to zalecana metoda. `state_dict` to s≈Çownik Pythonowy, kt√≥ry mapuje ka≈ºdƒÖ warstwƒô na jej tensory parametr√≥w (wagi i bias). ZapisujƒÖc tylko `state_dict`, zapisujemy tylko *nauczone parametry* modelu, a nie ca≈ÇƒÖ jego architekturƒô. Aby wczytaƒá model w ten spos√≥b, musimy najpierw zdefiniowaƒá (lub mieƒá dostƒôp do) klasy modelu z identycznƒÖ architekturƒÖ, a nastƒôpnie wczytaƒá do niej `state_dict`.\n",
        "2.  **Zapisywanie ca≈Çego modelu:** Ta metoda zapisuje ca≈ÇƒÖ architekturƒô modelu *wraz* z jego parametrami. Choƒá mo≈ºe wydawaƒá siƒô wygodniejsza, ma wady, takie jak sztywno≈õƒá (trudniej modyfikowaƒá model po wczytaniu) i potencjalne problemy z kompatybilno≈õciƒÖ miƒôdzy r√≥≈ºnymi wersjami PyTorcha lub je≈õli kod definiujƒÖcy model nie jest dostƒôpny. Z tego powodu zazwyczaj preferuje siƒô zapisywanie i wczytywanie samego `state_dict`.\n",
        "\n",
        "Poni≈ºej poka≈ºemy, jak zapisaƒá i wczytaƒá model, koncentrujƒÖc siƒô na zalecanej metodzie z u≈ºyciem `state_dict`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dab890ae"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os # Import os to manage file paths\n",
        "\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "input_dim = 10\n",
        "hidden_dim = 20\n",
        "output_dim = 5\n",
        "model_to_save = SimpleNN(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# Define the path to save the state dictionary\n",
        "model_save_path = 'simple_nn_state_dict.pth'\n",
        "\n",
        "# Save the state dictionary\n",
        "torch.save(model_to_save.state_dict(), model_save_path)\n",
        "\n",
        "print(f\"Model state dictionary saved to: {model_save_path}\")\n",
        "print(f\"File exists: {os.path.exists(model_save_path)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3db19018"
      },
      "source": [
        "# Create a new instance of the model with the same architecture\n",
        "model_to_load = SimpleNN(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# Define the path to the saved state dictionary\n",
        "model_load_path = 'simple_nn_state_dict.pth'\n",
        "\n",
        "# Load the state dictionary\n",
        "state_dict = torch.load(model_load_path)\n",
        "\n",
        "# Load the state dictionary into the new model instance\n",
        "model_to_load.load_state_dict(state_dict)\n",
        "\n",
        "# Set the model to evaluation mode (important if you are going to use it for inference)\n",
        "model_to_load.eval()\n",
        "\n",
        "print(f\"Model state dictionary loaded from: {model_load_path}\")\n",
        "print(\"Model is ready for use.\")\n",
        "\n",
        "# Optional: Verify weights (e.g., compare weights of the first layer)\n",
        "# print(\"Weights of the first layer after loading:\")\n",
        "# print(model_to_load.fc1.weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choƒá zalecanƒÖ metodƒÖ jest zapisywanie i wczytywanie tylko s≈Çownika stanu (`state_dict`), PyTorch umo≈ºliwia r√≥wnie≈º zapisanie ca≈Çego obiektu modelu, kt√≥ry zawiera zar√≥wno architekturƒô, jak i nauczone parametry. Robi siƒô to po prostu przekazujƒÖc ca≈Çy obiekt modelu do funkcji `torch.save()`.\n",
        "\n",
        "```python\n",
        "# Przyk≈Çad zapisania ca≈Çego modelu\n",
        "# model_to_save to instancja naszej klasy SimpleNN\n",
        "torch.save(model_to_save, 'simple_nn_full_model.pth')\n",
        "print(\"Ca≈Çy model zapisany do: simple_nn_full_model.pth\")\n",
        "```\n",
        "\n",
        "Aby wczytaƒá ca≈Çy model, u≈ºywamy funkcji `torch.load()`:\n",
        "\n",
        "```python\n",
        "# Przyk≈Çad wczytania ca≈Çego modelu\n",
        "loaded_full_model = torch.load('simple_nn_full_model.pth')\n",
        "loaded_full_model.eval() # Ustawienie modelu w tryb ewaluacji\n",
        "print(\"Ca≈Çy model wczytany.\")\n",
        "print(loaded_full_model) # Wydrukuje architekturƒô modelu\n",
        "```\n",
        "\n",
        "**Dlaczego zazwyczaj preferujemy zapisywanie `state_dict`?**\n",
        "\n",
        "1.  **Elastyczno≈õƒá:** Zapisanie tylko `state_dict` pozwala na wiƒôkszƒÖ elastyczno≈õƒá. Mo≈ºesz wczytaƒá parametry do modelu o zmodyfikowanej architekturze (np. doda≈Çe≈õ warstwƒô, zmieni≈Çe≈õ rozmiar warstwy, ale reszta jest taka sama), o ile klucze w `state_dict` pasujƒÖ do nazw parametr√≥w w nowym modelu. W przypadku zapisania ca≈Çego modelu, wczytanie wymaga dok≈Çadnego odtworzenia oryginalnego kodu klasy modelu.\n",
        "2.  **Jawno≈õƒá:** Zapisanie `state_dict` zmusza do jawnego zdefiniowania architektury modelu przed wczytaniem parametr√≥w. U≈Çatwia to zrozumienie, co dok≈Çadnie jest ≈Çadowane.\n",
        "3.  **Rozmiar pliku:** Plik z samym `state_dict` jest zazwyczaj mniejszy, poniewa≈º nie zawiera kodu definiujƒÖcego architekturƒô modelu.\n",
        "4.  **Kompatybilno≈õƒá:** Zapisanie ca≈Çego modelu jest bardziej podatne na problemy z kompatybilno≈õciƒÖ miƒôdzy r√≥≈ºnymi wersjami PyTorcha lub je≈õli struktura katalog√≥w/nazwy plik√≥w kodu modelu ulegnƒÖ zmianie.\n",
        "\n",
        "Zapisywanie ca≈Çego modelu jest wygodne do zapisania modeli na potrzeby wnioskowania w prostych skryptach, gdzie kod modelu jest ≈Çatwo dostƒôpny i niezmienny. Jednak dla bardziej z≈Ço≈ºonych projekt√≥w, trenowania roz≈Ço≈ºonego na wiele sesji lub gdy planuje siƒô modyfikacje architektur, zapisywanie `state_dict` jest bezpieczniejszƒÖ i bardziej elastycznƒÖ opcjƒÖ.\n"
      ],
      "metadata": {
        "id": "SHEWikIshzY6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f641f9c"
      },
      "source": [
        "# Demonstrate saving the entire model\n",
        "full_model_save_path = 'simple_nn_full_model.pth'\n",
        "torch.save(model_to_save, full_model_save_path) # Use the previously created model_to_save instance\n",
        "\n",
        "print(f\"Entire model saved to: {full_model_save_path}\")\n",
        "print(f\"File exists: {os.path.exists(full_model_save_path)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVOOO6cGLJs1"
      },
      "source": [
        "# Demonstrate loading the entire model with weights_only=False\n",
        "full_model_load_path = 'simple_nn_full_model.pth'\n",
        "loaded_full_model = torch.load(full_model_load_path, weights_only=False)\n",
        "\n",
        "# Set the loaded model to evaluation mode\n",
        "loaded_full_model.eval()\n",
        "\n",
        "print(f\"Entire model loaded from: {full_model_load_path}\")\n",
        "print(\"Loaded model architecture:\")\n",
        "print(loaded_full_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Wizja komputerowa"
      ],
      "metadata": {
        "id": "k9C-ujrjGHXk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### cuda setup"
      ],
      "metadata": {
        "id": "zW51EUFKG7FW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import itertools\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "\n",
        "SEED = 42 # he he 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "#tu zale≈ºnie od resource jakie macie na collabie - darmowe jest CPU, ale mo≈ºna sobie dokupiƒá zasoby obliczeniowe\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "0-edWSMaG84w"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pobieranie danych i dataloadery"
      ],
      "metadata": {
        "id": "kIJgKpjqHbdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transformacje: tensory + normalizacja (≈õrednia/odchylenie MNIST)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# pobranie danych (zapisze do folderu ./data)\n",
        "data_root = \"./data\"\n",
        "train_full = datasets.MNIST(root=data_root, train=True, download=True, transform=transform)\n",
        "test_ds    = datasets.MNIST(root=data_root, train=False, download=True, transform=transform)\n",
        "\n",
        "# podzia≈Ç danych na train/val\n",
        "val_size = 5000\n",
        "train_size = len(train_full) - val_size\n",
        "train_ds, val_ds = random_split(train_full, [train_size, val_size], generator=torch.Generator().manual_seed(SEED))\n",
        "\n",
        "BATCH_SIZE = 128 # mo≈ºna zmieniƒá na mniejszy - je≈õli nie bƒôdzie dzia≈Çaƒá (out of memory error), to TRZEBA zmieniƒá na mniejszy (16,32,64)\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "len(train_ds), len(val_ds), len(test_ds)"
      ],
      "metadata": {
        "id": "bMTee68VHdy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### podglƒÖd obraz√≥w i etykiet"
      ],
      "metadata": {
        "id": "0NX7vxU6Hzk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = next(iter(train_loader))\n",
        "images = images[:8]\n",
        "labels = labels[:8]\n",
        "\n",
        "fig, axes = plt.subplots(1, len(images), figsize=(10, 6))\n",
        "for ax, img, lab in zip(axes, images, labels):\n",
        "    ax.imshow(img.squeeze().numpy(), cmap=\"gray\")\n",
        "    ax.set_title(f\"Label: {lab.item()}\")\n",
        "    ax.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2adTZB9FHyT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prosty CNN"
      ],
      "metadata": {
        "id": "KU6Jh70AGrg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        # przypomnijcie sobie co ka≈ºda warstwa robi i po co ona jest.\n",
        "        # tu uwaga przy zmianach cyferek - przypomnijcie sobie matematyczne obliczenie. Musi siƒô zgadzaƒá pomiƒôdzy warstwami.\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool  = nn.MaxPool2d(2, 2)\n",
        "        # eksperymentujcie z warto≈õciami\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.fc1  = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2  = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "      # przypomnij sobie czym jest f. aktywacji ReLU\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleCNN().to(device)"
      ],
      "metadata": {
        "id": "Ve6rvlFBGpxD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pƒôtla uczenia z walidacjƒÖ"
      ],
      "metadata": {
        "id": "TZeTdh78G1K2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "# jak pu≈õcicie tƒÖ kom√≥rkƒô i d≈Çugo siƒô bƒôdzie krƒôciƒá bez ≈ºadnego wyniku na dole to JEST TO NORMALNE\n",
        "# nie stresujemy siƒô tylko czekamy....\n",
        "\n",
        "EPOCHS = 5\n",
        "lr = 1e-3\n",
        "optimizer = Adam(model.parameters(), lr=lr) # kim jest Adam?\n",
        "# czym jest cross entropy?\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def run_epoch(loader, train: bool):\n",
        "  # jak epochy wp≈ÇywajƒÖ na wynik?\n",
        "    epoch_loss, correct, total = 0.0, 0, 0\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(train):\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            if train:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        epoch_loss += loss.item() * xb.size(0)\n",
        "        preds = logits.argmax(1)\n",
        "        correct += (preds == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "    return epoch_loss / total, correct / total\n",
        "\n",
        "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
        "start = time.time()\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    tr_loss, tr_acc = run_epoch(train_loader, train=True)\n",
        "    va_loss, va_acc = run_epoch(val_loader, train=False)\n",
        "    history[\"train_loss\"].append(tr_loss); history[\"train_acc\"].append(tr_acc)\n",
        "    history[\"val_loss\"].append(va_loss);   history[\"val_acc\"].append(va_acc)\n",
        "    print(f\"Ep {epoch:02d}/{EPOCHS} | train loss {tr_loss:.4f} acc {tr_acc:.4f} | val loss {va_loss:.4f} acc {va_acc:.4f}\")\n",
        "print(f\"Czas treningu: {time.time()-start:.1f}s\")\n"
      ],
      "metadata": {
        "id": "j73uHbLcG4OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### monitorowanie przebiegu uczenia"
      ],
      "metadata": {
        "id": "vXycT9cGH4CX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(history[\"train_loss\"], label=\"train loss\")\n",
        "plt.plot(history[\"val_loss\"], label=\"val loss\")\n",
        "plt.xlabel(\"Epoka\")\n",
        "plt.ylabel(\"Strata\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(history[\"train_acc\"], label=\"train acc\")\n",
        "plt.plot(history[\"val_acc\"], label=\"val acc\")\n",
        "plt.xlabel(\"Epoka\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z_LoJjziH9tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ewaluacja"
      ],
      "metadata": {
        "id": "aKOw5C2tH-pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# na te≈õcie\n",
        "# spr√≥bujcie dopisaƒá jakƒÖ≈õ w≈ÇasnƒÖ wizualizacjƒô wynik√≥w\n",
        "model.eval()\n",
        "all_preds, all_targets = [], []\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb = xb.to(device, non_blocking=True)\n",
        "        logits = model(xb)\n",
        "        preds = logits.argmax(1).cpu().numpy()\n",
        "        all_preds.append(preds)\n",
        "        all_targets.append(yb.numpy())\n",
        "\n",
        "y_pred = np.concatenate(all_preds)\n",
        "y_true = np.concatenate(all_targets)\n",
        "\n",
        "test_acc = (y_pred == y_true).mean()\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(cm, interpolation='nearest')\n",
        "plt.title(\"Confusion Matrix (MNIST)\")\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(10)\n",
        "plt.xticks(tick_marks, tick_marks)\n",
        "plt.yticks(tick_marks, tick_marks)\n",
        "\n",
        "thresh = cm.max() / 2.0\n",
        "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "    plt.text(j, i, format(cm[i, j], 'd'),\n",
        "             horizontalalignment=\"center\",\n",
        "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.ylabel('Prawdziwa klasa')\n",
        "plt.xlabel('Predykcja')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(classification_report(y_true, y_pred, digits=4))"
      ],
      "metadata": {
        "id": "XEH5ybAbIA6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### wizualizacje predykcji"
      ],
      "metadata": {
        "id": "lyKEiLaJIGZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "images, labels = next(iter(test_loader))\n",
        "images, labels = images[:8], labels[:8]\n",
        "with torch.no_grad():\n",
        "    logits = model(images.to(device))\n",
        "preds = logits.argmax(1).cpu()\n",
        "\n",
        "fig, axes = plt.subplots(1, len(images), figsize=(12, 2.5))\n",
        "for ax, img, y, p in zip(axes, images, labels, preds):\n",
        "    ax.imshow(img.squeeze().numpy(), cmap=\"gray\")\n",
        "    ax.set_title(f\"y={y.item()} / p={int(p)}\")\n",
        "    ax.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WdX4eBB4IILh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### zapis i ≈Çadowanie modelu"
      ],
      "metadata": {
        "id": "eEPXq8JQIJHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = \"mnist_cnn.pt\"\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(f\"Model zapisany do: {os.path.abspath(save_path)}\")\n",
        "\n",
        "# do ≈Çadowania odkomentuj poni≈ºsze:\n",
        "# ≈Åadowanie:\n",
        "# model = SimpleCNN().to(device)\n",
        "# model.load_state_dict(torch.load(save_path, map_location=device))\n",
        "# model.eval()"
      ],
      "metadata": {
        "id": "Ny89TwLMIMyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LQfhPeWyHYoc"
      }
    }
  ]
}