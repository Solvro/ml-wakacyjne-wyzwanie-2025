{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# importowanie potrzebnych bibliotek"
      ],
      "metadata": {
        "id": "8WRFPDtt3P4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset, Subset\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import itertools\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "from torch.optim import Adam"
      ],
      "metadata": {
        "id": "0EKqVDtxa-ZS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wczytanie danych\n"
      ],
      "metadata": {
        "id": "u8Y9OJ8I3jI7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jRbqwL8K5KWI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podzielenie danych na zbiory testowe i treningowe. Dane które zostały pobrane przy pomocy tej metody trzeba było przerobić, ponieważ brakuje wymiaru kanału, który jest niezbędny dla sieci CNN."
      ],
      "metadata": {
        "id": "QjC0xSfW3mKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train = x_train[..., np.newaxis]\n",
        "x_test  = x_test[..., np.newaxis]\n",
        "\n",
        "x_train_tensor = torch.from_numpy(x_train)\n",
        "y_train_tensor = torch.from_numpy(y_train)\n",
        "\n",
        "x_train_tensor = torch.from_numpy(x_train).permute(0, 3, 1, 2).float()\n",
        "y_train_tensor = torch.from_numpy(y_train).long()\n",
        "\n",
        "x_test_tensor = torch.from_numpy(x_test).permute(0, 3, 1, 2).float()\n",
        "y_test_tensor = torch.from_numpy(y_test).long()\n",
        "\n",
        "print(x_train_tensor.shape)\n",
        "print(y_train_tensor.shape)\n",
        "print(x_test_tensor.shape)\n",
        "print(y_test_tensor.shape)\n",
        "\n",
        "plt.imshow(x_train[0], cmap='gray')\n",
        "plt.title(f\"Przykład cyfry: {y_train[0]}\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "4RJ0SzvWbC3L",
        "outputId": "7a7a4daf-bc9d-4d55-e105-630f7e16f926"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([60000, 1, 28, 28])\n",
            "torch.Size([60000])\n",
            "torch.Size([10000, 1, 28, 28])\n",
            "torch.Size([10000])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sprawdzenie danych, jakie liczby zawierają się w tym datasecie oraz ile ich jest"
      ],
      "metadata": {
        "id": "2bxM1Clw5NxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numbers = []\n",
        "for i in y_train:\n",
        "  if i not in numbers:\n",
        "    numbers.append(i)\n",
        "print(numbers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Nw4OOLzefgt",
        "outputId": "b782fdbb-0149-45d3-e42f-097c45f85673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[np.uint8(5), np.uint8(0), np.uint8(4), np.uint8(1), np.uint8(9), np.uint8(2), np.uint8(3), np.uint8(6), np.uint8(7), np.uint8(8)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN"
      ],
      "metadata": {
        "id": "mHHrztKx5Xuq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup"
      ],
      "metadata": {
        "id": "WrTH8zUI55PU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "5SvcWolwf3VC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zdefiniowanie sieci CNN"
      ],
      "metadata": {
        "id": "R7p3xSSH56c9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self, num_classes=10):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(1,32, kernel_size=3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.dropout = nn.Dropout(0.25)\n",
        "    self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "    self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = self.dropout(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "\n",
        "model = CNN().to(device)"
      ],
      "metadata": {
        "id": "ioRULvhgcLO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train_tensor.shape)\n",
        "print(y_train_tensor.shape)\n",
        "print(x_test_tensor.shape)\n",
        "print(y_test_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJ_ygstYMLEt",
        "outputId": "15b29058-5e05-4c9c-d8da-7a23d199c3c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([60000, 1, 28, 28])\n",
            "torch.Size([60000])\n",
            "torch.Size([10000, 1, 28, 28])\n",
            "torch.Size([10000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pętla uczenia, 5 epok, learning rate = 0.001, optymalizator Adam oraz kryterium entropii krzyżowej, które mówi nam jak bardzo przewidywania sieci różnią się od tego co ma faktycznie być."
      ],
      "metadata": {
        "id": "6qXwHJsT6DuO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalnie wytrenowana sieć pokazuje, że na zbiorze treningowym ma 93% accuracy oraz loss 0.2428 natomiast na zbiorze testowym wynik jest już nieco lepszy, mianowicie dokładność jest na poziomie 94,5% i loss wynosi 0.1811."
      ],
      "metadata": {
        "id": "qHUNNAi3_iX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 5\n",
        "lr = 1e-3\n",
        "optimizer = Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def run_epoch(X, y, train: bool):\n",
        "  epoch_loss, correct, total = 0.0, 0, 0\n",
        "  if train:\n",
        "    model.train()\n",
        "  else:\n",
        "    model.eval()\n",
        "\n",
        "  for i in range(0, len(X)):\n",
        "    xb = X[i]\n",
        "    yb = y[i]\n",
        "    xb = xb.unsqueeze(0)\n",
        "    yb = yb.unsqueeze(0)\n",
        "    xb, yb = xb.float().to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
        "    if train:\n",
        "      optimizer.zero_grad()\n",
        "    with torch.set_grad_enabled(train):\n",
        "      logits = model(xb)\n",
        "      loss = criterion(logits, yb)\n",
        "      if train:\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    epoch_loss += loss.item() * xb.size(0)\n",
        "    preds = logits.argmax(1)\n",
        "    correct += (preds == yb).sum().item()\n",
        "    total += yb.size(0)\n",
        "  return epoch_loss / total, correct / total\n",
        "\n",
        "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
        "start = time.time()\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    tr_loss, tr_acc = run_epoch(x_train_tensor, y_train_tensor, train=True)\n",
        "    va_loss, va_acc = run_epoch(x_test_tensor, y_test_tensor, train=False)\n",
        "    history[\"train_loss\"].append(tr_loss); history[\"train_acc\"].append(tr_acc)\n",
        "    history[\"val_loss\"].append(va_loss);   history[\"val_acc\"].append(va_acc)\n",
        "    print(f\"Ep {epoch:02d}/{EPOCHS} | train loss {tr_loss:.4f} acc {tr_acc:.4f} | val loss {va_loss:.4f} acc {va_acc:.4f}\")\n",
        "print(f\"Czas treningu: {time.time()-start:.1f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUBa4SZKh7Es",
        "outputId": "127cae28-b120-4a7a-e866-533e2c02f6ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 01/5 | train loss 0.2626 acc 0.9282 | val loss 0.2189 acc 0.9344\n",
            "Ep 02/5 | train loss 0.2447 acc 0.9327 | val loss 0.1751 acc 0.9472\n",
            "Ep 03/5 | train loss 0.2599 acc 0.9253 | val loss 0.1621 acc 0.9505\n",
            "Ep 04/5 | train loss 0.2483 acc 0.9282 | val loss 0.1719 acc 0.9481\n",
            "Ep 05/5 | train loss 0.2428 acc 0.9299 | val loss 0.1811 acc 0.9458\n",
            "Czas treningu: 4191.6s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Badanie - zwykła sieć vs CNN"
      ],
      "metadata": {
        "id": "YGHo6Gg7-b2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zdefiniowanie zestawów testowych oraz treningowych, standaryzacja tych zestawów oraz ograniczenie liczebności datasetu (5000 zbiór treningowy i 1000 zbiór testowy)"
      ],
      "metadata": {
        "id": "KaZnTKtK-g9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train_tensor = torch.from_numpy(x_train).float() / 255.0\n",
        "y_train_tensor = torch.from_numpy(y_train).long()\n",
        "\n",
        "x_test_tensor = torch.from_numpy(x_test).float() / 255.0\n",
        "y_test_tensor = torch.from_numpy(y_test).long()\n",
        "\n",
        "batch_size = 64\n",
        "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "test_dataset  = TensorDataset(x_test_tensor, y_test_tensor)\n",
        "\n",
        "train_subset = Subset(train_dataset, range(5000))\n",
        "test_subset  = Subset(test_dataset, range(1000))\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_subset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "rCR0MZSgUMPi"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prosta sieć neuronowa"
      ],
      "metadata": {
        "id": "PIFjEWUL-2ZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Y4UfouOFM1KJ"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SimpleNN(input_size=28*28, hidden_size=128, output_size=10).to(device)"
      ],
      "metadata": {
        "id": "_v8LKJuNcicD"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pętla uczenia 5 epok, learning rate = 0.001, optymalizator Adam oraz kryterium entropii krzyżowej."
      ],
      "metadata": {
        "id": "O-Mpgjb7-9b-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uczenie prostej sieci neuronowej, finalny wynik to 97% dokładności oraz 0.0693 loss dla zbioru treningowego. Natomiast gorzej wypada finalny test na zbiorze testowym, ponieważ dokładność spada do 90% a loss to już 0.5623."
      ],
      "metadata": {
        "id": "HUd8Q9-KAKKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 5\n",
        "lr = 1e-3\n",
        "optimizer = Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def run_epoch(loader, train: bool):\n",
        "    epoch_loss, correct, total = 0.0, 0, 0\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(train):\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            if train:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        epoch_loss += loss.item() * xb.size(0)\n",
        "        preds = logits.argmax(1)\n",
        "        correct += (preds == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "    return epoch_loss / total, correct / total\n",
        "\n",
        "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
        "start = time.time()\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    tr_loss, tr_acc = run_epoch(train_loader, train=True)\n",
        "    va_loss, va_acc = run_epoch(test_loader, train=False)\n",
        "    history[\"train_loss\"].append(tr_loss); history[\"train_acc\"].append(tr_acc)\n",
        "    history[\"val_loss\"].append(va_loss);   history[\"val_acc\"].append(va_acc)\n",
        "    print(f\"Ep {epoch:02d}/{EPOCHS} | train loss {tr_loss:.4f} acc {tr_acc:.4f} | \"\n",
        "          f\"val loss {va_loss:.4f} acc {va_acc:.4f}\")\n",
        "print(f\"Czas treningu: {time.time()-start:.1f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc3uoLRlgo9d",
        "outputId": "056b42e5-e3ed-46a1-f8c7-6cca9feececc"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 01/5 | train loss 1.0553 acc 0.7570 | val loss 0.5723 acc 0.8350\n",
            "Ep 02/5 | train loss 0.3867 acc 0.8974 | val loss 0.4077 acc 0.8790\n",
            "Ep 03/5 | train loss 0.2957 acc 0.9200 | val loss 0.3596 acc 0.8910\n",
            "Ep 04/5 | train loss 0.2503 acc 0.9298 | val loss 0.3419 acc 0.8950\n",
            "Ep 05/5 | train loss 0.2164 acc 0.9432 | val loss 0.3289 acc 0.8960\n",
            "Czas treningu: 0.8s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definiowanie zbioru testowego i treningowego dla sieci CNN"
      ],
      "metadata": {
        "id": "_4OlgLJSAuo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train_cnn = x_train[..., np.newaxis]\n",
        "x_test_cnn  = x_test[..., np.newaxis]\n",
        "\n",
        "x_train_tensor_cnn = torch.from_numpy(x_train_cnn).permute(0, 3, 1, 2).float()\n",
        "y_train_tensor_cnn = torch.from_numpy(y_train).long()\n",
        "\n",
        "x_test_tensor_cnn = torch.from_numpy(x_test_cnn).permute(0, 3, 1, 2).float()\n",
        "y_test_tensor_cnn = torch.from_numpy(y_test).long()\n",
        "\n",
        "batch_size = 64\n",
        "train_dataset_cnn = TensorDataset(x_train_tensor_cnn, y_train_tensor_cnn)\n",
        "test_dataset_cnn  = TensorDataset(x_test_tensor_cnn, y_test_tensor_cnn)\n",
        "\n",
        "train_subset_cnn = Subset(train_dataset_cnn, range(5000))\n",
        "test_subset_cnn  = Subset(test_dataset_cnn, range(1000))\n",
        "\n",
        "train_loader_cnn = DataLoader(train_subset_cnn, batch_size=batch_size, shuffle=True)\n",
        "test_loader_cnn  = DataLoader(test_subset_cnn, batch_size=batch_size, shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "OxcbEzRSmNMC"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool  = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.fc1  = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2  = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model_cnn = SimpleCNN().to(device)"
      ],
      "metadata": {
        "id": "2vXrvUo7BHsb"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pętla uczenia dla sieci CNN"
      ],
      "metadata": {
        "id": "BMJIb6swA1rt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sieć CNN radzi sobie lepiej na zbiorze treningowym, ale wypada podobnie na zbiorze testowym co zwykła sieć."
      ],
      "metadata": {
        "id": "Lpug3IVICtJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 5\n",
        "lr = 1e-3\n",
        "optimizer = Adam(model_cnn.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def run_epoch(loader, train: bool):\n",
        "    epoch_loss, correct, total = 0.0, 0, 0\n",
        "    if train:\n",
        "        model_cnn.train()\n",
        "    else:\n",
        "        model_cnn.eval()\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(train):\n",
        "            logits = model_cnn(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            if train:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        epoch_loss += loss.item() * xb.size(0)\n",
        "        preds = logits.argmax(1)\n",
        "        correct += (preds == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "    return epoch_loss / total, correct / total\n",
        "\n",
        "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
        "start = time.time()\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    tr_loss, tr_acc = run_epoch(train_loader_cnn, train=True)\n",
        "    va_loss, va_acc = run_epoch(test_loader_cnn, train=False)\n",
        "    history[\"train_loss\"].append(tr_loss); history[\"train_acc\"].append(tr_acc)\n",
        "    history[\"val_loss\"].append(va_loss);   history[\"val_acc\"].append(va_acc)\n",
        "    print(f\"Ep {epoch:02d}/{EPOCHS} | train loss {tr_loss:.4f} acc {tr_acc:.4f} | val loss {va_loss:.4f} acc {va_acc:.4f}\")\n",
        "print(f\"Czas treningu: {time.time()-start:.1f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RV5CKssVcgsf",
        "outputId": "2d1d889d-1676-4f26-96df-29506f5d3cf8"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 01/5 | train loss 2.0828 acc 0.7050 | val loss 0.2295 acc 0.9360\n",
            "Ep 02/5 | train loss 0.2359 acc 0.9308 | val loss 0.1694 acc 0.9390\n",
            "Ep 03/5 | train loss 0.1683 acc 0.9448 | val loss 0.1105 acc 0.9620\n",
            "Ep 04/5 | train loss 0.1291 acc 0.9570 | val loss 0.1150 acc 0.9610\n",
            "Ep 05/5 | train loss 0.0962 acc 0.9678 | val loss 0.1119 acc 0.9630\n",
            "Czas treningu: 21.8s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Badanie - zmiana parametrów sieci CNN"
      ],
      "metadata": {
        "id": "aDbSWNL8Dcp2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do badania efektywności sieci używam cross entropy loss oraz accuracy, które odpowiednio mówią nam jak bardzo sieć się myli względem tego co powinno być naprawdę oraz procent trafionych dobrze liczb na tle całego datasetu."
      ],
      "metadata": {
        "id": "QMexQVRPDooP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zwiększenie szerokości oraz wysokości sieci"
      ],
      "metadata": {
        "id": "lNMfhMWPF8dz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.pool  = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.fc1  = nn.Linear(256 * 3 * 3, 128)\n",
        "        self.fc2  = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model_cnn2 = SimpleCNN().to(device)"
      ],
      "metadata": {
        "id": "d3wnR-9oDirG"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 5\n",
        "lr = 1e-3\n",
        "optimizer = Adam(model_cnn2.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def run_epoch(loader, train: bool):\n",
        "    epoch_loss, correct, total = 0.0, 0, 0\n",
        "    if train:\n",
        "        model_cnn2.train()\n",
        "    else:\n",
        "        model_cnn2.eval()\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(train):\n",
        "            logits = model_cnn2(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            if train:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        epoch_loss += loss.item() * xb.size(0)\n",
        "        preds = logits.argmax(1)\n",
        "        correct += (preds == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "    return epoch_loss / total, correct / total\n",
        "\n",
        "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
        "start = time.time()\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    tr_loss, tr_acc = run_epoch(train_loader_cnn, train=True)\n",
        "    va_loss, va_acc = run_epoch(test_loader_cnn, train=False)\n",
        "    history[\"train_loss\"].append(tr_loss); history[\"train_acc\"].append(tr_acc)\n",
        "    history[\"val_loss\"].append(va_loss);   history[\"val_acc\"].append(va_acc)\n",
        "    print(f\"Ep {epoch:02d}/{EPOCHS} | train loss {tr_loss:.4f} acc {tr_acc:.4f} | val loss {va_loss:.4f} acc {va_acc:.4f}\")\n",
        "print(f\"Czas treningu: {time.time()-start:.1f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1JEgDPlER6f",
        "outputId": "a9512cdb-7e9d-4059-e108-3f48999f61c0"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 01/5 | train loss 1.5800 acc 0.6500 | val loss 0.2269 acc 0.9310\n",
            "Ep 02/5 | train loss 0.2243 acc 0.9332 | val loss 0.1275 acc 0.9570\n",
            "Ep 03/5 | train loss 0.1489 acc 0.9546 | val loss 0.1362 acc 0.9640\n",
            "Ep 04/5 | train loss 0.1190 acc 0.9608 | val loss 0.0694 acc 0.9770\n",
            "Ep 05/5 | train loss 0.1168 acc 0.9644 | val loss 0.1200 acc 0.9600\n",
            "Czas treningu: 89.5s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Widać, że statystyki treningu znacznie się poprawiły, tak samo jak statystyki z testowania tej sieci, które mówią, że loss znacznie się zmniejszył i zwiększyła się również dokładność. Sieć dobrze zgaduje liczby i jest pewna swoich decyzji. Wydłużył się znacznie czas treningu tej sieci."
      ],
      "metadata": {
        "id": "EWKXQOy6HqNS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zmniejszenie wysokości i szerokości sieci"
      ],
      "metadata": {
        "id": "-tdCC9KBIjXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 4, kernel_size=3, padding=1)\n",
        "        self.pool  = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.fc1  = nn.Linear(4 * 14 * 14, 128)\n",
        "        self.fc2  = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model_cnn3 = SimpleCNN().to(device)"
      ],
      "metadata": {
        "id": "Q6qJ7jv4InEX"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 5\n",
        "lr = 1e-3\n",
        "optimizer = Adam(model_cnn3.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def run_epoch(loader, train: bool):\n",
        "    epoch_loss, correct, total = 0.0, 0, 0\n",
        "    if train:\n",
        "        model_cnn3.train()\n",
        "    else:\n",
        "        model_cnn3.eval()\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(train):\n",
        "            logits = model_cnn3(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            if train:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        epoch_loss += loss.item() * xb.size(0)\n",
        "        preds = logits.argmax(1)\n",
        "        correct += (preds == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "    return epoch_loss / total, correct / total\n",
        "\n",
        "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
        "start = time.time()\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    tr_loss, tr_acc = run_epoch(train_loader_cnn, train=True)\n",
        "    va_loss, va_acc = run_epoch(test_loader_cnn, train=False)\n",
        "    history[\"train_loss\"].append(tr_loss); history[\"train_acc\"].append(tr_acc)\n",
        "    history[\"val_loss\"].append(va_loss);   history[\"val_acc\"].append(va_acc)\n",
        "    print(f\"Ep {epoch:02d}/{EPOCHS} | train loss {tr_loss:.4f} acc {tr_acc:.4f} | val loss {va_loss:.4f} acc {va_acc:.4f}\")\n",
        "print(f\"Czas treningu: {time.time()-start:.1f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2J25TiyIyTS",
        "outputId": "e974b48f-07d6-4baf-af40-25af1ad2d6a1"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 01/5 | train loss 1.4052 acc 0.6698 | val loss 0.3949 acc 0.8760\n",
            "Ep 02/5 | train loss 0.4094 acc 0.8714 | val loss 0.2718 acc 0.9120\n",
            "Ep 03/5 | train loss 0.3010 acc 0.8990 | val loss 0.2227 acc 0.9280\n",
            "Ep 04/5 | train loss 0.2348 acc 0.9236 | val loss 0.2357 acc 0.9290\n",
            "Ep 05/5 | train loss 0.1882 acc 0.9400 | val loss 0.2137 acc 0.9380\n",
            "Czas treningu: 3.2s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zmniejszenie szerokości i wysokości sieci nie spowodowało znacznego pogorszenia sieci, lekko zwiększył się loss oraz zmniejszyła dokładność ale dalej sieć jest dobra."
      ],
      "metadata": {
        "id": "OMfK_XZFJsXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Badanie wpływu BatchNorm oraz wartości Dropout**"
      ],
      "metadata": {
        "id": "q69VHM2UKhvd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dodanie BatchNorm"
      ],
      "metadata": {
        "id": "54ZrJFh7LsqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.pool  = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.fc1  = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2  = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model_cnn4 = SimpleCNN().to(device)"
      ],
      "metadata": {
        "id": "tgcRTxfXKmgP"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 5\n",
        "lr = 1e-3\n",
        "optimizer = Adam(model_cnn4.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def run_epoch(loader, train: bool):\n",
        "    epoch_loss, correct, total = 0.0, 0, 0\n",
        "    if train:\n",
        "        model_cnn4.train()\n",
        "    else:\n",
        "        model_cnn4.eval()\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(train):\n",
        "            logits = model_cnn4(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            if train:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        epoch_loss += loss.item() * xb.size(0)\n",
        "        preds = logits.argmax(1)\n",
        "        correct += (preds == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "    return epoch_loss / total, correct / total\n",
        "\n",
        "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
        "start = time.time()\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    tr_loss, tr_acc = run_epoch(train_loader_cnn, train=True)\n",
        "    va_loss, va_acc = run_epoch(test_loader_cnn, train=False)\n",
        "    history[\"train_loss\"].append(tr_loss); history[\"train_acc\"].append(tr_acc)\n",
        "    history[\"val_loss\"].append(va_loss);   history[\"val_acc\"].append(va_acc)\n",
        "    print(f\"Ep {epoch:02d}/{EPOCHS} | train loss {tr_loss:.4f} acc {tr_acc:.4f} | val loss {va_loss:.4f} acc {va_acc:.4f}\")\n",
        "print(f\"Czas treningu: {time.time()-start:.1f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKgFATfzLx6u",
        "outputId": "158ed7d8-4c47-4954-8288-d792d78707d2"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 01/5 | train loss 0.6960 acc 0.7874 | val loss 0.2277 acc 0.9430\n",
            "Ep 02/5 | train loss 0.2042 acc 0.9434 | val loss 0.1486 acc 0.9530\n",
            "Ep 03/5 | train loss 0.1396 acc 0.9590 | val loss 0.1127 acc 0.9630\n",
            "Ep 04/5 | train loss 0.1001 acc 0.9700 | val loss 0.0953 acc 0.9710\n",
            "Ep 05/5 | train loss 0.0946 acc 0.9726 | val loss 0.0765 acc 0.9770\n",
            "Czas treningu: 30.0s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dodanie BatchNorm poskutkowało tym, że statystyki samego treningu jak i testowania sieci polepszyły się znacznie, zmniejszając lossa i zwiększając dokładność."
      ],
      "metadata": {
        "id": "wfqufyPyMdUB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zmniejszenie, zwiększenie dropout"
      ],
      "metadata": {
        "id": "qsWtucPQN6V4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool  = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.001)\n",
        "        self.fc1  = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2  = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model_cnn5 = SimpleCNN().to(device)"
      ],
      "metadata": {
        "id": "SGs7edzwN_L8"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 5\n",
        "lr = 1e-3\n",
        "optimizer = Adam(model_cnn5.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def run_epoch(loader, train: bool):\n",
        "    epoch_loss, correct, total = 0.0, 0, 0\n",
        "    if train:\n",
        "        model_cnn5.train()\n",
        "    else:\n",
        "        model_cnn5.eval()\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(train):\n",
        "            logits = model_cnn5(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            if train:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        epoch_loss += loss.item() * xb.size(0)\n",
        "        preds = logits.argmax(1)\n",
        "        correct += (preds == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "    return epoch_loss / total, correct / total\n",
        "\n",
        "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
        "start = time.time()\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    tr_loss, tr_acc = run_epoch(train_loader_cnn, train=True)\n",
        "    va_loss, va_acc = run_epoch(test_loader_cnn, train=False)\n",
        "    history[\"train_loss\"].append(tr_loss); history[\"train_acc\"].append(tr_acc)\n",
        "    history[\"val_loss\"].append(va_loss);   history[\"val_acc\"].append(va_acc)\n",
        "    print(f\"Ep {epoch:02d}/{EPOCHS} | train loss {tr_loss:.4f} acc {tr_acc:.4f} | val loss {va_loss:.4f} acc {va_acc:.4f}\")\n",
        "print(f\"Czas treningu: {time.time()-start:.1f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcqkRjuTOIdE",
        "outputId": "7f985a02-63e9-4b0b-cced-fc296e3a7be8"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 01/5 | train loss 2.3013 acc 0.7408 | val loss 0.2406 acc 0.9220\n",
            "Ep 02/5 | train loss 0.1595 acc 0.9514 | val loss 0.1540 acc 0.9430\n",
            "Ep 03/5 | train loss 0.0839 acc 0.9730 | val loss 0.1450 acc 0.9370\n",
            "Ep 04/5 | train loss 0.0574 acc 0.9820 | val loss 0.1158 acc 0.9560\n",
            "Ep 05/5 | train loss 0.0418 acc 0.9858 | val loss 0.1017 acc 0.9630\n",
            "Czas treningu: 21.6s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zmniejszenie dropout poprawia działanie sieci, zmniejszając loss oraz dokładność"
      ],
      "metadata": {
        "id": "JSaKgIbkXM1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zwiększenie Dropout"
      ],
      "metadata": {
        "id": "nL-65dNcXVHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool  = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.9)\n",
        "        self.fc1  = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2  = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model_cnn6 = SimpleCNN().to(device)"
      ],
      "metadata": {
        "id": "fhwl31rxOUto"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 5\n",
        "lr = 1e-3\n",
        "optimizer = Adam(model_cnn6.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def run_epoch(loader, train: bool):\n",
        "    epoch_loss, correct, total = 0.0, 0, 0\n",
        "    if train:\n",
        "        model_cnn6.train()\n",
        "    else:\n",
        "        model_cnn6.eval()\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(train):\n",
        "            logits = model_cnn6(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            if train:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        epoch_loss += loss.item() * xb.size(0)\n",
        "        preds = logits.argmax(1)\n",
        "        correct += (preds == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "    return epoch_loss / total, correct / total\n",
        "\n",
        "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
        "start = time.time()\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    tr_loss, tr_acc = run_epoch(train_loader_cnn, train=True)\n",
        "    va_loss, va_acc = run_epoch(test_loader_cnn, train=False)\n",
        "    history[\"train_loss\"].append(tr_loss); history[\"train_acc\"].append(tr_acc)\n",
        "    history[\"val_loss\"].append(va_loss);   history[\"val_acc\"].append(va_acc)\n",
        "    print(f\"Ep {epoch:02d}/{EPOCHS} | train loss {tr_loss:.4f} acc {tr_acc:.4f} | val loss {va_loss:.4f} acc {va_acc:.4f}\")\n",
        "print(f\"Czas treningu: {time.time()-start:.1f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVwIUzOgOV_s",
        "outputId": "c96ccf29-1206-4688-9f8c-e0d2ec2417a6"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 01/5 | train loss 7.9193 acc 0.1126 | val loss 2.2976 acc 0.1170\n",
            "Ep 02/5 | train loss 2.2817 acc 0.1516 | val loss 2.2587 acc 0.2560\n",
            "Ep 03/5 | train loss 2.2466 acc 0.1778 | val loss 2.2160 acc 0.3500\n",
            "Ep 04/5 | train loss 2.2121 acc 0.2018 | val loss 2.1160 acc 0.4750\n",
            "Ep 05/5 | train loss 2.1400 acc 0.2280 | val loss 1.9602 acc 0.5890\n",
            "Czas treningu: 21.8s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zwiększenie dropout sprawiło, że sieć jakby uczyła się wolniej, prawdopodobnie zwiększenie liczby epok poskutkowałoby tym, że finalnie sieć nauczyłaby się poprawnie klasyfikować liczby."
      ],
      "metadata": {
        "id": "nrtUAlnfXYBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Podsumowanie"
      ],
      "metadata": {
        "id": "v3KeQak8X1ch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zwykła sieć neuronowa nieznacznie się różni skutecznością od CNN akurat dla tego problemu, podejrzewam, że przy bardziej złożonych klasyfikacjach CNN byłaby nieporównywalnie lepsza. Badanie zmian parametrów sieci CNN pokazało, że przy odpowiednim doborze parametrów można jeszcze bardziej zwiększyć skuteczność sieci a niewłaściwie dobrane parametry mogą tylko zaszkodzić."
      ],
      "metadata": {
        "id": "yPrYvj8GX4cI"
      }
    }
  ]
}