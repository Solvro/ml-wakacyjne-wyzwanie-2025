{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "w4dU1aeGMaRS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30a36312-40ff-402b-a5d0-ac6486b1f797"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install langchain sentence-transformers faiss-cpu pypdf transformers torch langchain-community PyPDF2"
      ],
      "metadata": {
        "id": "wZAIM5C86Im_"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline#, AutoModelForSeq2SeqLM\n",
        "from transformers import AutoModelForQuestionAnswering # tego będziemy używać\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# MOŻESZ IMPORTOWAĆ INNE BIBLIOTEKI, ALE PAMIĘTAJ O ICH INSTALACJI W WIERSZU POWYŻEJ !!!"
      ],
      "metadata": {
        "id": "Xy2C1La8OHxI"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Import pliku pdf"
      ],
      "metadata": {
        "id": "KSVuCplKUthO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_files = [ # wybrałem 3 artykuły o analize rynku mieszkaniowego\n",
        "    '/content/drive/MyDrive/ml-zadanie4/housing_crisis_1.pdf',\n",
        "    '/content/drive/MyDrive/ml-zadanie4/housing_crisis_2.pdf',\n",
        "    '/content/drive/MyDrive/ml-zadanie4/housing_crisis_3.pdf'\n",
        "]\n",
        "\n",
        "all_data = []\n",
        "\n",
        "# podział na fragmenty\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=100,\n",
        "    length_function=len,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")\n",
        "\n",
        "for path in pdf_files:\n",
        "    print(f\"Loading {path}...\")\n",
        "    loader = PyPDFLoader(path)\n",
        "    pages = loader.load()\n",
        "    print(f\"  -> loaded {len(pages)} pages\")\n",
        "\n",
        "    docs = text_splitter.split_documents(pages)\n",
        "    print(f\"  -> split into {len(docs)} chunks\")\n",
        "\n",
        "    all_data.extend(docs)\n",
        "\n",
        "print(f\"Załadowano {len(all_data)} fragmentów\")"
      ],
      "metadata": {
        "id": "5bHW616tUtFX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66870bc3-3c6e-4057-c060-0518a6cfdace"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading /content/drive/MyDrive/ml-zadanie4/housing_crisis_1.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 12 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 45 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 47 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 59 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 78 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 148 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> loaded 11 pages\n",
            "  -> split into 135 chunks\n",
            "Loading /content/drive/MyDrive/ml-zadanie4/housing_crisis_2.pdf...\n",
            "  -> loaded 36 pages\n",
            "  -> split into 453 chunks\n",
            "Loading /content/drive/MyDrive/ml-zadanie4/housing_crisis_3.pdf...\n",
            "  -> loaded 13 pages\n",
            "  -> split into 166 chunks\n",
            "Załadowano 754 fragmentów\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pypdf pokazał problemy, ale najwyżej część dokumentu pozostanie niewczytana."
      ],
      "metadata": {
        "id": "O9cYpxFyNBDF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Wektorowa baza danych"
      ],
      "metadata": {
        "id": "Z1qdeyLXUi9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# wybrałem model sentence-transformers/all-MiniLM-L6-v2\n",
        "\n",
        "embed_model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embed_model_name)\n",
        "db = FAISS.from_documents(docs, embeddings)"
      ],
      "metadata": {
        "id": "pTX-QSOQUjSp"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zapis do pliku\n",
        "\n",
        "db.save_local(\"vector_db\")"
      ],
      "metadata": {
        "id": "_CoItM8vVN90"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sprawdzenie semantyczne (top 3 wyniki)\n",
        "\n",
        "def retrieve_relevant_docs(query, top_k=3):\n",
        "    similar_docs = db.similarity_search(query, top_k)  # top 3 wyniki\n",
        "    return similar_docs\n",
        "\n",
        "query = \"What are the causes of housing crisis?\"\n",
        "similar_docs = retrieve_relevant_docs(query)\n",
        "\n",
        "\n",
        "\n",
        "for doc in similar_docs:\n",
        "    print(doc.page_content + \"...\\n---\")"
      ],
      "metadata": {
        "id": "34YaDSw7FNNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94e861de-7fe0-442b-a7f9-e31bb512039e"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "systemic challenges:  \n",
            "• Incomes not keeping up with housing costs, and  \n",
            "• The severe shortage of homes affordable and available to households with the \n",
            "lowest incomes.  \n",
            "The Gap Between Incomes and Housing Costs \n",
            "A major cause of housing instability is the fundamental mismatch between growing...\n",
            "---\n",
            "A major cause of housing instability is the fundamental mismatch between growing \n",
            "housing costs, and comparatively stagnant incomes for people with the lowest incomes. \n",
            "Over one-third of extremely low-income renters are in the labor force (35%), while the...\n",
            "---\n",
            "essential to provide stable, affordable housing for those who the private sector cannot \n",
            "serve on their own: households with the lowest incomes. \n",
            "Underlying Causes of the Affordable Housing Crisis  \n",
            "The United States is experiencing an affordable housing and homelessness crisis...\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ciekawy output, wydaje się być poprawny, czasami trochę niekompletny."
      ],
      "metadata": {
        "id": "A2k9u5yJHQsb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Model generacji"
      ],
      "metadata": {
        "id": "uHc9xyICV4Ng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# wybrałem model deepset/bert-base-cased-squad2\n",
        "\n",
        "model_name = 'deepset/bert-base-cased-squad2'\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "GDpMxx3ZVUY4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b9a436f-b3d7-4b48-b5e6-9eb985236073"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/bert-base-cased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_pipeline = pipeline(task=\"question-answering\", model=model, tokenizer=tokenizer, max_length=512)\n",
        "llm = HuggingFacePipeline(pipeline=qa_pipeline)\n",
        "\n",
        "def answer_question(query):\n",
        "    relevant_docs = retrieve_relevant_docs(query)\n",
        "\n",
        "    answers = []\n",
        "\n",
        "    for doc in relevant_docs:\n",
        "        result = qa_pipeline(question=query, context=doc.page_content)\n",
        "        answers.append({\n",
        "            'answer': result['answer'],\n",
        "            'score': result['score'],\n",
        "            'context': doc\n",
        "        })\n",
        "\n",
        "    best_answer = max(answers, key=lambda x: x['score'])\n",
        "\n",
        "    if best_answer['score'] < 0.2:\n",
        "      return \"Try asking a different question.\"\n",
        "    if best_answer['score'] < 0.3:\n",
        "      return best_answer['answer'] + \" (This answer may be highly inaccurate!)\"\n",
        "\n",
        "    return best_answer['answer']\n",
        "\n",
        "# pytanie 1\n",
        "query = \"What causes the housing crisis?\"\n",
        "answer = answer_question(query)\n",
        "print(f\"AI Assistant: {answer}\")\n",
        "\n",
        "# pytanie 2\n",
        "query = \"Who suffers from the housing crisis?\"\n",
        "answer = answer_question(query)\n",
        "print(f\"AI Assistant: {answer}\")\n",
        "\n",
        "# głupie pytanie\n",
        "query = \"What time is it?\"\n",
        "answer = answer_question(query)\n",
        "print(f\"AI Assistant: {answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S80w-09TIeWZ",
        "outputId": "34fc084a-8ab7-43ad-eb5f-2edb99fe4a67"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI Assistant: growing \n",
            "housing costs\n",
            "AI Assistant: households with the \n",
            "lowest incomes\n",
            "AI Assistant: Try asking a different question.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Pętla z \"chatbotem\""
      ],
      "metadata": {
        "id": "NLaebknbDdbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Hello! Type 'end' to exit.\\n\")\n",
        "\n",
        "while True:\n",
        "  query = input(\"User: \")\n",
        "  if query == \"end\":\n",
        "    print(\"Goodbye!\")\n",
        "    break\n",
        "  answer = answer_question(query)\n",
        "  print(f\"AI Assistant: {answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pdrN-yeDdFF",
        "outputId": "d755ecd2-6fd2-4e2e-a5b4-50769406b8b9"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! Type 'end' to exit.\n",
            "\n",
            "User: how much affordable housing shortage is it?\n",
            "AI Assistant: 7.3 million\n",
            "User: what country is experiencing the housing crisis?\n",
            "AI Assistant: United States\n",
            "User: who are most low-income renters?\n",
            "AI Assistant: seniors\n",
            "User: what are the available tax credits?\n",
            "AI Assistant: Low-Income Housing Tax Credit \n",
            "Program\n",
            "User: what percent of income is paid for housing?\n",
            "AI Assistant: half\n",
            "User: who needs to be supplied housing?\n",
            "AI Assistant: lowest-income renters\n",
            "User: what needs to be done?\n",
            "AI Assistant: improvements to federal programs\n",
            "User: how much people experience homelessness\n",
            "AI Assistant: millions of the lowest-income and most \n",
            "marginalized households\n",
            "User: end\n",
            "Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Krótka analiza serii pytań:\n",
        "\n",
        "---\n",
        "\n",
        "User: how much affordable housing shortage is it?\n",
        "AI Assistant: 7.3 million\n",
        "\n",
        "Ten model dobrze sobie radzi z szukaniem liczb\n",
        "\n",
        "---\n",
        "\n",
        "User: what country is experiencing the housing crisis?\n",
        "AI Assistant: United States\n",
        "\n",
        "Tutaj widać, że dane są ograniczone do tych trzech dokumentów\n",
        "\n",
        "---\n",
        "\n",
        "User: what percent of income is paid for housing?\n",
        "AI Assistant: half\n",
        "\n",
        "Zapewne wyjęte z kontekstu\n",
        "\n",
        "---\n",
        "\n",
        "User: what needs to be done?\n",
        "AI Assistant: improvements to federal programs\n",
        "\n",
        "Ostro\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "p3QW13BAK96y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Podsumowanie\n",
        "\n",
        "- Model poprawnie odpowiada na pytania, pod warunkiem że są dość dokładnie sformułowane\n",
        "\n",
        "- Mechanizm oceny podobieństwa pomaga w ocenie, czy model zaczyna zmyślać odpowiedzi\n",
        "\n",
        "- Pracujemy na bardzo ograniczonej bazie danych, przez co wyniki są jakie są\n",
        "\n",
        "- Odpowiedzi to raczej lekko przetworzone fragmenty zdań wyrwane z kontekstu, który był powiązany z pytaniem według modelu embeddingowego\n",
        "\n",
        "- Następnym razem wybrałbym inny temat do analizy, ten jest dość rozległy w danych i ma trochę \"humanistyczną\" treść, tj. mało konkretów, sporo długich zdań\n",
        "\n",
        "- Niestety moje wcześniejsze próby z modelami w języku polskim nie udały się"
      ],
      "metadata": {
        "id": "zD6XeGwNL_Tz"
      }
    }
  ]
}